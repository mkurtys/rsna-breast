{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment this cell to run the notebook with TPUs üëáüèª"
      ],
      "metadata": {
        "id": "Vq4xUH5aervK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# ! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# ! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev"
      ],
      "metadata": {
        "id": "G5H6DRrtervM",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5gZOg8Sq89C",
        "outputId": "344d2948-ad44-47fb-9133-f6da4f95282b",
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 13 22:18:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment/Dataset configuration\n",
        "\n",
        "## TLDR\n",
        "`kaggle.json` - Kaggle API key\n",
        "\n",
        "wandb - if enabled enter api key in notebook (need to change - troublesome)\n",
        "\n",
        "## Details\n",
        "Basicaly 3 options:\n",
        "* Kaggle environement\n",
        "* Google colab environment\n",
        "* Other\n",
        "\n",
        "When running on kaggle, kaggle datasets are available since runtime is ready.\n",
        "Otherwise one have to download dataset - downloading from Kaggle is good option.\n",
        "**In order to download kaggle dataset one need his own API key**\n",
        "\n",
        "\n",
        "Original Kaggle DICOM dataset ~300GB needs preprocessing.\n",
        "`kaggle competitions download -c rsna-breast-cancer-detection`\n",
        "\n",
        "Currently I use community-processed or self-processed training data.\n",
        "Kaggle supports creating own private datasets up to 20GB each one."
      ],
      "metadata": {
        "id": "885GuIVA6XCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    IS_KAGGLE = True\n",
        "except ImportError:\n",
        "    IS_KAGGLE = False\n",
        "    \n",
        "if IS_KAGGLE:\n",
        "    DATASET_PATH = \"/kaggle/input/\"\n",
        "else:\n",
        "    DATASET_PATH = os.path.expanduser(\"~/rsna-breast\")\n",
        "    !mkdir -p {DATASET_PATH}"
      ],
      "metadata": {
        "trusted": true,
        "id": "7EyqMVYw6XCK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO add wandb api key\n",
        "try:\n",
        "    from google.colab import files\n",
        "    if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")):\n",
        "        files.upload()\n",
        "        #2. Series of commands to set-up for download\n",
        "        !ls -lha kaggle.json\n",
        "        !mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n",
        "        !cp kaggle.json ~/.kaggle/ # move the key to the folder\n",
        "        #3. giving rw access (if 401-nathorized)\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "except ImportError:\n",
        "    pass\n",
        "if not IS_KAGGLE:\n",
        "    !pip install -q kaggle # installing the kaggle package\n",
        "    !mkdir -p ~/kaggle/\n",
        "    !kaggle datasets download -d awsaf49/rsna-bcd-roi-1024x512-png-v2-dataset -p {DATASET_PATH}\n",
        "    !unzip -q {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset.zip -d {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset\n",
        "    !rm {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "kQNAy38OervN",
        "outputId": "912c6883-a24d-4e59-a7af-3773722fe3dc",
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49cb3670-0b89-41fc-a390-a52dc64d3ef2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49cb3670-0b89-41fc-a390-a52dc64d3ef2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 66 Jan 13 22:20 kaggle.json\n",
            "Downloading rsna-bcd-roi-1024x512-png-v2-dataset.zip to /root/rsna-breast\n",
            " 59% 6.55G/11.1G [05:09<03:46, 21.5MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and Imports"
      ],
      "metadata": {
        "id": "By3AjokVervP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install --upgrade timm\n",
        "! pip install --upgrade wandb\n",
        "! pip install --upgrade torchmetrics\n",
        "! pip install --upgrade pytorch-lightning\n",
        "! pip install --upgrade albumentations\n",
        "! pip install --upgrade wandb\n",
        "! pip install --upgrade opencv-python"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "RJTFkspLervP",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Iterator, Iterable, Optional, Sequence, List, TypeVar, Generic, Sized, Union\n",
        "\n",
        "import wandb\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
        "\n",
        "import sklearn.model_selection\n",
        "\n",
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize, Affine\n",
        ")\n",
        "import albumentations.augmentations.transforms\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# import warnings\n",
        "# warnings.simplefilter('ignore')\n"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "TUHr4YJbervQ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config File and Wandb"
      ],
      "metadata": {
        "id": "IGp3ez88ervR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WandbMode:\n",
        "    OFFLINE = \"offline\"\n",
        "    ONLINE = \"online\"\n",
        "    DISABLED = \"disabled\"\n",
        "    \n",
        "class WandbConfig:\n",
        "    # change to disabled/offline if not needed\n",
        "    mode = WandbMode.ONLINE\n",
        "    project = \"rsna-breast\"\n",
        "    name = None\n",
        "    save_dir = None\n",
        "    log_model = \"all\"\n",
        "    # use specific model artifact to download and start with \n",
        "    # TODO add also local model checkpoints!\n",
        "    model_artifact_checkpoint_reference = None\n",
        "    \n",
        "\n",
        "class Config:\n",
        "    train_bs=16\n",
        "    valid_bs=16\n",
        "    model_name=\"tf_efficientnetv2_s\"\n",
        "    model_params = {\n",
        "      \"drop_rate\": 0.2,\n",
        "      \"drop_path_rate\": 0.2\n",
        "    }\n",
        "    overwrite_checkpoint_hparams=True\n",
        "    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_256/train_images_processed_cv2_256\"\n",
        "    #train_imgs_path = \"/kaggle/input/rsna-breast-png-roi/train_images/512/\"\n",
        "    #train_imgs_path = \"/kaggle/input/rsna-cut-off-empty-space-from-images\"\n",
        "    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_vl_512/train_images_processed_cv2_vl_512\"\n",
        "    train_imgs_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train_images\"\n",
        "    \n",
        "    # True if image data folders have structure <patient_id>/<image_id>.png\n",
        "    # False if image data have structure <patient_id>_<image_id>.png\n",
        "    data_has_patient_folder_sturcture = True\n",
        "    \n",
        "    #train_csv_path = \"/kaggle/input/rsna-breast-cancer-detection/train.csv\"\n",
        "    train_csv_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train.csv\"\n",
        "    \n",
        "    test_imgs_path = f\"{DATASET_PATH}/rsna-breast-png-roi/test_images/256/\"\n",
        "    \n",
        "    accelerator=\"auto\"   # auto, gpu or cpu\n",
        "    num_devices=1        # TODO - parallel computations\n",
        "\n",
        "    resize_aspect_ratio = None\n",
        "     \n",
        "    use_pretrained=True\n",
        "    epochs_count = 20\n",
        "    splits_count = 1\n",
        "    warmup_lr = 1e-6\n",
        "    warmup_epochs = 1\n",
        "    lr=5e-3\n",
        "    t_max= 8 \n",
        "    min_lr= 1e-5\n",
        "    weight_decay=1e-5\n",
        "    # weight of cancer examples in loss function\n",
        "    pos_weight = 1\n",
        "    # upsample cancers to percent\n",
        "    positive_upsample_to_percent = 0.3\n",
        "    # mixed-precision\n",
        "    precision=16 if torch.cuda.is_available() else 32 \n",
        "    \n",
        "    # dataloaders\n",
        "    dataloader_workers_count = 4\n",
        "    # aux_targets = ['BIRADS'] # not used\n",
        "    \n",
        "    debug=False\n",
        "    debug_data_use_only_percent=0.001\n",
        "    \n",
        "\n",
        "def config_class_to_dict(config_class):\n",
        "    # add recursion if needed\n",
        "    return { k:v for k,v in Config.__dict__.items() if not k.startswith(\"__\") }"
      ],
      "metadata": {
        "id": "Jd7ZTkBBervS",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Config.train_imgs_path)\n",
        "print(Path(Config.train_imgs_path).exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSYdv1rKervT",
        "outputId": "743b73db-35c6-42ab-e889-43ed952ab58f",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset/train_images\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = None\n",
        "\n",
        "wandb_logger = None\n",
        "wandb_artifact = None\n",
        "wandb_artifact_dir = None\n",
        "if WandbConfig.mode != WandbMode.DISABLED:\n",
        "    wb_key = None\n",
        "    if IS_KAGGLE:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "    else:\n",
        "        wb_key = os.environ.get(\"WANDB_API_KEY\")\n",
        "    # wandb should get key fron environment\n",
        "    wandb.login(key=wb_key)\n",
        "\n",
        "    wandb_logger = WandbLogger(\n",
        "        project=WandbConfig.project,\n",
        "        name = WandbConfig.name,\n",
        "        offline = True if WandbConfig.mode == WandbMode.OFFLINE else False,\n",
        "        save_dir = WandbConfig.save_dir,\n",
        "        log_model = WandbConfig.log_model,\n",
        "        group='vision',\n",
        "        job_type='train',\n",
        "        config=config_class_to_dict(Config)\n",
        "    )\n",
        "\n",
        "    if WandbConfig.mode == WandbMode.ONLINE and WandbConfig.model_artifact_checkpoint_reference:\n",
        "        artifact = wandb.run.use_artifact(\n",
        "            WandbConfig.model_artifact_checkpoint_reference, type=\"model\")\n",
        "        artifact_dir = artifact.download()\n",
        "        checkpoint_path = Path(artifact_dir) / \"model.ckpt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orXIS0zgervU",
        "outputId": "aafdf05f-5b1a-426c-e6d0-81007655b1ef",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichal-kurtys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/wandb.py:322: UserWarning: Providing log_model=all requires wandb version >= 0.10.22 for logging associated model metadata.\n",
            "Hint: Upgrade with `pip install --upgrade wandb`.\n",
            "  rank_zero_warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not used, but efficientnet v2 do not use weight decay\n",
        "# for some parameters\n",
        "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):\n",
        "            # print(name, 'no decay')\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            # print(name, 'decay')\n",
        "            decay.append(param)\n",
        "    return [\n",
        "        {'params': no_decay, 'weight_decay': 0.},\n",
        "        {'params': decay, 'weight_decay': weight_decay}]"
      ],
      "metadata": {
        "id": "liQqkQMCervV",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_opt_and_scheduler():\n",
        "    opt = torch.optim.SGD([torch.tensor(1)], lr=Config.lr)\n",
        "    scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n",
        "                                                     factor=Config.warmup_lr/Config.lr,\n",
        "                                                     total_iters=20)\n",
        "    scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                opt, \n",
        "                T_max=Config.t_max,\n",
        "                eta_min=Config.min_lr\n",
        "                )\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(opt,\n",
        "                                                      schedulers=[scheduler1, scheduler2],\n",
        "                                                      milestones=[Config.warmup_epochs])\n",
        "    return opt, scheduler"
      ],
      "metadata": {
        "id": "rQ3Pmd1HervW",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def visualise_scheduler(opt, scheduler, steps=Config.epochs_count, lr=1):\n",
        "    lrs = []\n",
        "    for _ in range(100):\n",
        "        opt.step()\n",
        "        lrs.append(scheduler.get_last_lr())\n",
        "        scheduler.step()\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(lrs)\n",
        "    fig.show()\n",
        "opt, scheduler = create_test_opt_and_scheduler()\n",
        "visualise_scheduler(opt,\n",
        "                    scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "78B6xkRFervW",
        "outputId": "20a9e386-d3ca-43ea-dee1-a1840280ea15",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def probabilistic_f1(labels, preds, beta=1):\n",
        "    \"\"\"\n",
        "    Function taken from Awsaf's notebook:\n",
        "    https://www.kaggle.com/code/awsaf49/rsna-bcd-efficientnet-tf-tpu-1vm-train\n",
        "    \"\"\"\n",
        "    eps = 1e-5\n",
        "    preds = preds.clip(0, 1)\n",
        "    y_true_count = labels.sum()\n",
        "    ctp = preds[labels==1].sum()\n",
        "    cfp = preds[labels==0].sum()\n",
        "    beta_squared = beta * beta\n",
        "    c_precision = ctp / (ctp + cfp + eps)\n",
        "    c_recall = ctp / (y_true_count + eps)\n",
        "    if (c_precision > 0 and c_recall > 0):\n",
        "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + eps)\n",
        "        return result\n",
        "    else:\n",
        "        return 0.0"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "HSo25zKvervX",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class - for loading in data"
      ],
      "metadata": {
        "id": "E08Abs2WervY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_batch(batch, figsize=(16,10), n_cols=2, maximgs=None, cmap=\"bone\"):\n",
        "    if isinstance(batch, tuple) or isinstance(batch, list):\n",
        "        if len(batch) == 2:\n",
        "            imgs, targets = batch\n",
        "        else:\n",
        "            imgs, targets, filenames = batch\n",
        "    else:\n",
        "        imgs = batch\n",
        "        targets = None\n",
        "    #targets = targets.numpy().squeeze()\n",
        "    \n",
        "    n = imgs.shape[0] if not maximgs else min(imgs.shape[0], maximgs)\n",
        "    n_rows = math.ceil(n / n_cols)\n",
        "    \n",
        "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
        "    for i in range(n):\n",
        "        x = i % n_cols\n",
        "        y = i // n_cols\n",
        "        ax=axs[y,x]\n",
        "        im = imgs[i, :, :, :].squeeze()\n",
        "        im = im.numpy().transpose((1,2,0)).squeeze()\n",
        "        ax.imshow(im, cmap=cmap)\n",
        "        if targets is not None:\n",
        "            ax.set_title(f'Target {targets[i]}', fontsize=10)\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "    fig.tight_layout()\n",
        "    fig.show() "
      ],
      "metadata": {
        "id": "ZW7CKrzzervY",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_images_to_max_dim(batch, pad_value = -1):\n",
        "    \"Pad images with values if there are images of different size in batch\"\n",
        "    # consider non-symmetrical padding\n",
        "    \n",
        "    # print([img.shape for img in batch])\n",
        "    max_h = max([img.shape[-2] for img in batch])\n",
        "    max_w = max([img.shape[-1] for img in batch])\n",
        "    # pad all images in batch\n",
        "    padded_batch = []\n",
        "    for i, img in enumerate(batch):\n",
        "        pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0\n",
        "        diff_w = max_w - img.shape[-1]\n",
        "        diff_h = max_h - img.shape[-2]\n",
        "        if diff_w > 0:\n",
        "            pad_left = diff_w//2\n",
        "            pad_right = diff_w - pad_left\n",
        "        if diff_h > 0:\n",
        "            pad_top = diff_h//2\n",
        "            pad_bottom = diff_h - pad_top\n",
        "        if any([pad_left, pad_right, pad_top, pad_bottom]):\n",
        "            padded_img = torch.nn.functional.pad(img,\n",
        "                            (pad_left, pad_right, pad_top, pad_bottom),\n",
        "                            value=pad_value)\n",
        "            padded_batch.append(padded_img)\n",
        "        else:\n",
        "            padded_batch.append(img)\n",
        "        \n",
        "    return padded_batch\n",
        "\n",
        "def mixed_collate_imgs_fn(batch):\n",
        "    batch_soft_cpy = [x for x in batch]\n",
        "    batch_imgs = [x[0] for x in batch]\n",
        "    \n",
        "    padded_batch = []\n",
        "    padded_batch_imgs = pad_images_to_max_dim(batch_imgs)\n",
        "    for b_item, pad_img in zip(batch, padded_batch_imgs):\n",
        "        padded_batch.append( (pad_img, *b_item[1:]))\n",
        "        \n",
        "    return torch.utils.data.default_collate(padded_batch)\n",
        "\n",
        "# supports both 3-channels PNGs, and single-channel 16 bit PNG\n",
        "class RSNAData(Dataset):\n",
        "    def __init__(self, df,\n",
        "                 img_folder,\n",
        "                 resize_dim=None,\n",
        "                 resize_aspect_ratio=None,\n",
        "                 transform=None,\n",
        "                 is_test=False,\n",
        "                 has_patient_folder_sturcture=False,\n",
        "                 extension=\"png\",\n",
        "                 return_filepath=False):\n",
        "        \n",
        "        assert resize_dim is not None and resize_aspect_ratio is not None\n",
        "        self.df = df\n",
        "        self.is_test = is_test\n",
        "        self.transform = transform\n",
        "        self.img_folder = img_folder\n",
        "        self.resize_dim = resize_dim\n",
        "        self.resize_aspect_ratio=resize_aspect_ratio\n",
        "        self.extension = extension\n",
        "        self.has_patient_folder_sturcture=has_patient_folder_sturcture\n",
        "        self.return_filepath=return_filepath\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        row = df.loc[idx, :]\n",
        "        \n",
        "        if self.has_patient_folder_sturcture:\n",
        "            img_path = os.path.join(self.img_folder, str(row[\"patient_id\"]), f\"{row['image_id']}.{self.extension}\")\n",
        "        else:\n",
        "            img_path = os.path.join(self.img_folder, f\"{row['image_id']}.{self.extension}\")\n",
        "        \n",
        "        # need anydepth to load 16bit grayscale png\n",
        "        # don't know if pngs are bgr or rgb\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"No image {img_path} found\")\n",
        "        # cast to np float, as torch can't into uint16\n",
        "        if self.resize_dim:\n",
        "            img = cv2.resize(img, self.resize_dim)\n",
        "        if self.resize_aspect_ratio:\n",
        "            img = cv2.resize(img, img.shape*self.resize_aspect_ratio)\n",
        "                    \n",
        "        # convert to RGB for pretrained networks\n",
        "        # maybe we will move it to the different place\n",
        "        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[0] == 1):\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # should we really normalize here? Why not in transforms?\n",
        "        # again there are problems (there are?) with 16bits PNGS\n",
        "        img = img.astype(np.float32)\n",
        "        img_max = np.amax(img)\n",
        "        img_min = np.amin(img)\n",
        "        img_range = img_max-img_min\n",
        "        if img_range > 0:\n",
        "            img = (img-img_min)/img_range\n",
        "     \n",
        "        if self.transform:\n",
        "            img = self.transform(image=img)['image']      \n",
        "        \n",
        "        # shouldn't be done in transforms?\n",
        "        img = torch.tensor(img, dtype=torch.float)\n",
        "        \n",
        "        if not self.is_test:\n",
        "            target = self.df['cancer'][idx]\n",
        "            target = torch.tensor(target, dtype=torch.float32)\n",
        "            if self.return_filepath:\n",
        "                return img, target, img_path\n",
        "            return (img, target)\n",
        "        if self.return_filepath:\n",
        "            return img, target\n",
        "        return (img)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "qumIr-oFervZ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test pading\n",
        "y= pad_images_to_max_dim([torch.tensor(np.random.rand(256,256)),\n",
        "                          torch.tensor(np.random.randn(100,50))])\n",
        "[x.shape for x in y]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqX9dNsDerva",
        "outputId": "8bc0c9ae-1049-4004-b9d8-29f674197ff1",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([256, 256]), torch.Size([256, 256])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data and pass it onto the training function\n",
        "df = pd.read_csv(Config.train_csv_path)\n",
        "df['img_name'] = df['patient_id'].astype(str) + \"/\" + df['image_id'].astype(str) + \".png\"\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "qHIlDM9yerva",
        "outputId": "895d48f3-aed8-4a0f-9c1a-4627755ed7f9",
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   site_id  patient_id    image_id laterality view   age  cancer  biopsy  \\\n",
              "0        1       11745  1185186309          R  MLO  69.0       0       0   \n",
              "1        1        5143  1295053298          R  MLO  56.0       0       0   \n",
              "2        2       42315   633141467          L   CC  62.0       0       0   \n",
              "3        2        9625    58889842          R  MLO  69.0       0       0   \n",
              "4        2       50518  1443816057          L   CC  65.0       0       0   \n",
              "\n",
              "   invasive  BIRADS  implant density  machine_id  difficult_negative_case  \\\n",
              "0         0     1.0        0       B          49                    False   \n",
              "1         0     1.0        0       B         210                    False   \n",
              "2         0     0.0        0     NaN          21                     True   \n",
              "3         0     NaN        0     NaN          21                    False   \n",
              "4         0     NaN        0     NaN          29                    False   \n",
              "\n",
              "                                          image_path  width  height  \\\n",
              "0  /tmp/dataset/part0/train_images/11745/11851863...    452     859   \n",
              "1  /tmp/dataset/part3/train_images/5143/129505329...    427     850   \n",
              "2  /tmp/dataset/part2/train_images/42315/63314146...    411     857   \n",
              "3  /tmp/dataset/part4/train_images/9625/58889842.png    306    1022   \n",
              "4  /tmp/dataset/part3/train_images/50518/14438160...    516     938   \n",
              "\n",
              "               img_name  \n",
              "0  11745/1185186309.png  \n",
              "1   5143/1295053298.png  \n",
              "2   42315/633141467.png  \n",
              "3     9625/58889842.png  \n",
              "4  50518/1443816057.png  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-019f6aac-b30e-42fe-a410-f928e8a5b274\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site_id</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>laterality</th>\n",
              "      <th>view</th>\n",
              "      <th>age</th>\n",
              "      <th>cancer</th>\n",
              "      <th>biopsy</th>\n",
              "      <th>invasive</th>\n",
              "      <th>BIRADS</th>\n",
              "      <th>implant</th>\n",
              "      <th>density</th>\n",
              "      <th>machine_id</th>\n",
              "      <th>difficult_negative_case</th>\n",
              "      <th>image_path</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>img_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>11745</td>\n",
              "      <td>1185186309</td>\n",
              "      <td>R</td>\n",
              "      <td>MLO</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>49</td>\n",
              "      <td>False</td>\n",
              "      <td>/tmp/dataset/part0/train_images/11745/11851863...</td>\n",
              "      <td>452</td>\n",
              "      <td>859</td>\n",
              "      <td>11745/1185186309.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5143</td>\n",
              "      <td>1295053298</td>\n",
              "      <td>R</td>\n",
              "      <td>MLO</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>B</td>\n",
              "      <td>210</td>\n",
              "      <td>False</td>\n",
              "      <td>/tmp/dataset/part3/train_images/5143/129505329...</td>\n",
              "      <td>427</td>\n",
              "      <td>850</td>\n",
              "      <td>5143/1295053298.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>42315</td>\n",
              "      <td>633141467</td>\n",
              "      <td>L</td>\n",
              "      <td>CC</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>True</td>\n",
              "      <td>/tmp/dataset/part2/train_images/42315/63314146...</td>\n",
              "      <td>411</td>\n",
              "      <td>857</td>\n",
              "      <td>42315/633141467.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>9625</td>\n",
              "      <td>58889842</td>\n",
              "      <td>R</td>\n",
              "      <td>MLO</td>\n",
              "      <td>69.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>False</td>\n",
              "      <td>/tmp/dataset/part4/train_images/9625/58889842.png</td>\n",
              "      <td>306</td>\n",
              "      <td>1022</td>\n",
              "      <td>9625/58889842.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>50518</td>\n",
              "      <td>1443816057</td>\n",
              "      <td>L</td>\n",
              "      <td>CC</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "      <td>False</td>\n",
              "      <td>/tmp/dataset/part3/train_images/50518/14438160...</td>\n",
              "      <td>516</td>\n",
              "      <td>938</td>\n",
              "      <td>50518/1443816057.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-019f6aac-b30e-42fe-a410-f928e8a5b274')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-019f6aac-b30e-42fe-a410-f928e8a5b274 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-019f6aac-b30e-42fe-a410-f928e8a5b274');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Image Augmentations"
      ],
      "metadata": {
        "id": "eRlsr6nZerva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_augments = Compose([\n",
        "    HorizontalFlip(p=0.5),\n",
        "    VerticalFlip(p=0.5),\n",
        "    Affine(rotate=(-10,10), p=0.2),\n",
        "    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n",
        "    ToTensorV2(p=1.0)\n",
        "],p=1.)\n",
        "    \n",
        "valid_augments = Compose([\n",
        "    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n",
        "    ToTensorV2(p=1.0)\n",
        "], p=1.)"
      ],
      "metadata": {
        "id": "RjJyqfU2ervb",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - have exhaustive sampler with upsampling positive examples\n",
        "# in contrast to random sampler which is not necessary exhaustive\n",
        "# of course maybe we do not need exhaustiveness\n",
        "class PositiveNegativeIndicesSampler(Sampler[int]):\n",
        "\n",
        "    def __init__(self, positive_indices, negative_indices, num_samples,\n",
        "                 negatives_per_batch, batch_size,\n",
        "                 generator=None) -> None:\n",
        "        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n",
        "                num_samples <= 0:\n",
        "            raise ValueError(\"num_samples should be a positive integer \"\n",
        "                             \"value, but got num_samples={}\".format(num_samples))\n",
        "\n",
        "        self.positive_indices = positive_indices\n",
        "        self.negative_indices = negative_indices\n",
        "        self.num_samples = num_samples\n",
        "        self.generator = generator\n",
        "\n",
        "    def __iter__(self) -> Iterator[int]:\n",
        "        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n",
        "        yield from iter(rand_tensor.tolist())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.num_samples"
      ],
      "metadata": {
        "id": "x140jOLrervb",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO refactor variable naming\n",
        "def class_weight(class_array, upsample_cancer_to_percent):\n",
        "    \"Returns weights for array of target classes so that positive examples are upsampled to given percent\"\n",
        "    \n",
        "    has_cancer = class_array.astype(np.float64)\n",
        "    \n",
        "    dataset_len = len(has_cancer)\n",
        "    cancer_len = len(has_cancer[has_cancer > 0])\n",
        "    non_cancer_len = dataset_len-cancer_len\n",
        "    \n",
        "    x = non_cancer_len*upsample_cancer_to_percent / (cancer_len -  upsample_cancer_to_percent*cancer_len)\n",
        "    \n",
        "    has_cancer[has_cancer > 0] = x\n",
        "    has_cancer[has_cancer <= 0 ] = 1\n",
        "    return has_cancer"
      ],
      "metadata": {
        "id": "PDdEi7-Fervb",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Class using `pl.LightningModule` ‚ö°Ô∏è"
      ],
      "metadata": {
        "id": "f1iFj-n_ervc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "class_weight( np.concatenate((np.zeros(9),np.ones(1))),  0.5)"
      ],
      "metadata": {
        "id": "WuMcJqYJervc",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSNAModel(pl.LightningModule):\n",
        "    def __init__(self, base_model,\n",
        "                 lr,\n",
        "                 warmup_lr,\n",
        "                 warmup_epochs,\n",
        "                 t_max,\n",
        "                 min_lr,\n",
        "                 weight_decay,\n",
        "                 features_size=None,\n",
        "                 pos_weight=None,   \n",
        "                ):\n",
        "        super().__init__()\n",
        "        # save_hyperparameters() is used to specify which init arguments should \n",
        "        # be saved in the checkpoint file to be used to instantiate the model\n",
        "        # from the checkpoint later.\n",
        "        self.save_hyperparameters(ignore=[\"base_model\", \"features_size\"])\n",
        "        # Model Architecture\n",
        "        self.model = base_model\n",
        "        classes_count = 1\n",
        "        if not features_size:\n",
        "            features_size = base_model(torch.randn(1, 3, 512, 512)).shape[-1]\n",
        "        self.fc = nn.Linear(features_size, classes_count)\n",
        "        \n",
        "        # Loss functions\n",
        "        # CHECKME - I believe that positive weight could not work here after re-loading model\n",
        "        pos_weight=None if pos_weight is None else torch.tensor([pos_weight], dtype=torch.float32)\n",
        "        self.train_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        self.valid_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Metric\n",
        "        self.roc_auc = torchmetrics.classification.BinaryAUROC()\n",
        "        self.pr_curve = torchmetrics.classification.BinaryPrecisionRecallCurve()\n",
        "        self.valid_stat_scores = torchmetrics.classification.BinaryStatScores()\n",
        "        self.train_stat_scores = torchmetrics.classification.BinaryStatScores()\n",
        "        self.binary_f1 = torchmetrics.F1Score(task='binary')\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n",
        "                               weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "        scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n",
        "                                                         factor=self.hparams.warmup_lr/self.hparams.lr,\n",
        "                                                         total_iters=self.hparams.warmup_epochs)\n",
        "        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "                        opt, \n",
        "                        T_max=self.hparams.t_max,\n",
        "                        eta_min=self.hparams.min_lr\n",
        "                     )\n",
        "        sch = torch.optim.lr_scheduler.SequentialLR(opt,\n",
        "                                                    schedulers=[scheduler1, scheduler2],\n",
        "                                                    milestones=[self.hparams.warmup_epochs])\n",
        "        \n",
        "        return [opt], [sch]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.model(x)\n",
        "        output = self.fc(features)\n",
        "        return output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs = batch[0]\n",
        "        target = batch[1].unsqueeze(-1)\n",
        "        \n",
        "        out = self(imgs)\n",
        "        positive_outcomes_mean = out[out>=0].mean()\n",
        "        negative_outcomes_mean = out[out<0].mean()\n",
        "        cancer_percent = target[target>0].shape[0]/target.shape[0]\n",
        "        \n",
        "        predictions = F.sigmoid(out)\n",
        "        \n",
        "        # shall we compute every x epochs? (but we will loose epoch metrics!)\n",
        "        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores(predictions, target)\n",
        "        train_loss = self.train_loss(out, target)\n",
        "        to_log = {'train/loss': train_loss.item(),\n",
        "                  'train/positive_outcomes_mean': positive_outcomes_mean.item(),\n",
        "                  'train/negative_outcomes_mean': negative_outcomes_mean.item(),\n",
        "                  'train/accuracy': ((true_positives+true_negatives)/(true_positives+false_positives+true_negatives+false_negatives)).item(),\n",
        "                  'train/precision': (true_positives/(true_positives+false_positives)).item(),\n",
        "                  'train/recall': (true_positives/(true_positives+false_negatives)).item()\n",
        "                 }\n",
        "        self.log_dict(to_log)\n",
        "        return train_loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs = batch[0]\n",
        "        target = batch[1].unsqueeze(-1)\n",
        "        \n",
        "        out = self(imgs)\n",
        "        \n",
        "        valid_loss = self.valid_loss(out, target)\n",
        "        \n",
        "        predictions = F.sigmoid(out)\n",
        "          \n",
        "        f1_current = self.binary_f1(predictions, target)\n",
        "        self.roc_auc(predictions, target)\n",
        "        self.valid_stat_scores(predictions, target)\n",
        "        self.log_dict({\"valid/loss\": valid_loss.item(),\n",
        "                       \"valid/f1\": f1_current.item()})     \n",
        "        return valid_loss\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores.compute()\n",
        "        to_log = {\n",
        "                  'train/precision_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
        "                  'train/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
        "                  'train/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n",
        "        self.log_dict(to_log) \n",
        "        self.train_stat_scores.reset()\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        true_positives, false_positives, true_negatives, false_negatives, sup = self.valid_stat_scores.compute()\n",
        "        to_log = {'valid/roc_auc_epoch': self.roc_auc.compute().item(),\n",
        "                  'valid/f1_epoch': self.binary_f1.compute().item(),\n",
        "                  'valid/precision_epoch': (true_positives/(true_positives+false_positives)).item(),\n",
        "                  'valid/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
        "                  'valid/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n",
        "        self.log_dict(to_log) \n",
        "        self.binary_f1.reset()\n",
        "        self.roc_auc.reset()\n",
        "        self.valid_stat_scores.reset()\n",
        "        \n",
        "    # def test_step(self, batch, batch_idx):\n",
        "    #    loss, acc = self._shared_eval_step(batch, batch_idx)\n",
        "    #    metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
        "    #    self.log_dict(metrics)\n",
        "    #    return metrics\n",
        "    \n",
        "\n",
        "\n",
        "config_hparams_dict = dict(\n",
        "    lr=Config.lr,\n",
        "    warmup_lr=Config.warmup_lr,\n",
        "    warmup_epochs=Config.warmup_epochs,\n",
        "    t_max=Config.t_max,\n",
        "    min_lr=Config.min_lr,\n",
        "    weight_decay=Config.weight_decay,\n",
        "    pos_weight=Config.pos_weight)"
      ],
      "metadata": {
        "id": "ewQLdSfdervd",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tb_logger = TensorBoardLogger(save_dir=\"tb_logs\", name=\"rsna-breast-model\")\n",
        "loggers = [x for x in [wandb_logger, tb_logger] if x is not None]\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\",\n",
        "    monitor=\"valid/f1_epoch\",\n",
        "    filename=\"ckpt_epoch={epoch:02d}_validf1={valid/f1_epoch:02.0f}\",\n",
        "    auto_insert_metric_name=False,\n",
        "    save_top_k=3,\n",
        "    mode=\"max\",\n",
        "    save_last=True,\n",
        "    every_n_epochs=3)"
      ],
      "metadata": {
        "id": "SCHzzGNTervd",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main loop"
      ],
      "metadata": {
        "id": "imsqGEdPerve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# add stratified k folds when ready\n",
        "kfold = sklearn.model_selection.StratifiedShuffleSplit(\n",
        "    n_splits=Config.splits_count, train_size=0.8)\n",
        "for fold_, (train_idx, valid_idx) in enumerate(kfold.split(df, df['cancer'].values)):\n",
        "    print(f\"{'='*40} Fold: {fold_} / {Config.splits_count-1} {'='*40}\")\n",
        "\n",
        "\n",
        "    train_df = df.loc[train_idx].reset_index(drop=True).copy()\n",
        "    valid_df = df.loc[valid_idx].reset_index(drop=True).copy()\n",
        "        \n",
        "    if Config.debug:\n",
        "        train_df = train_df.iloc[:int(len(train_df)*Config.debug_data_use_only_percent), :]\n",
        "        valid_df = train_df.iloc[:int(len(valid_df)*Config.debug_data_use_only_percent), :]\n",
        "    \n",
        "    train_rows_weight = class_weight(train_df[\"cancer\"].values, Config.positive_upsample_to_percent)   \n",
        "\n",
        "    print(f\"Train df len {len(train_df)}, cancers {train_df['cancer'].sum()}, percent {train_df['cancer'].sum()/len(train_df)}\")\n",
        "    print(f\"Valid df len {len(valid_df)}, cancers {valid_df['cancer'].sum()}, percent {valid_df['cancer'].sum()/len(valid_df)}\")        \n",
        "    print(f\"In train dataset cancer will be upsampled to percent {train_rows_weight[train_rows_weight>1].sum()/train_rows_weight.sum()}\")\n",
        "    \n",
        "    train_dataset = RSNAData(\n",
        "        df = train_df,\n",
        "        img_folder = Config.train_imgs_path,\n",
        "        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
        "        transform=train_augments\n",
        "    )\n",
        "    valid_dataset = RSNAData(\n",
        "        df = valid_df,\n",
        "        img_folder = Config.train_imgs_path,\n",
        "        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
        "        transform = valid_augments\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.train_bs,\n",
        "        sampler=torch.utils.data.sampler.WeightedRandomSampler(train_rows_weight, len(train_df)),\n",
        "        num_workers=Config.dataloader_workers_count,\n",
        "        collate_fn=mixed_collate_imgs_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=Config.train_bs,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.dataloader_workers_count,\n",
        "        collate_fn=mixed_collate_imgs_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    # CHECKME after introducing k-fold start with fresh copy of base model\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        if Config.overwrite_checkpoint_hparams:\n",
        "            hparams = config_hparams_dict\n",
        "        else:\n",
        "            hparams = {}\n",
        "        model = RSNAModel.load_from_checkpoint(checkpoint_path, \n",
        "                                              **hparams)\n",
        "    else:   \n",
        "        # it turns that i have lower LB than other kagglers using efficientnet.\n",
        "        # i have forgotten to set the drop path rate. Here is the fixed:\n",
        "        # efficientnet_b2(pretrained=True, drop_rate = 0.3, drop_path_rate = 0.2)\n",
        "        model_name = Config.model_name\n",
        "        base_model = timm.create_model(model_name, \n",
        "                                       pretrained=Config.use_pretrained,\n",
        "                                       **Config.model_params\n",
        "                                      ) \n",
        "        model = RSNAModel(base_model,\n",
        "                          **config_hparams_dict)\n",
        "    \n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=Config.epochs_count,\n",
        "        accelerator=Config.accelerator,\n",
        "        devices=Config.num_devices,\n",
        "        precision=Config.precision,\n",
        "        logger = loggers,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "    trainer.fit(model, train_loader, valid_loader)\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "gAEMsNwmerve",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RSNAData(\n",
        "    df = df,\n",
        "    img_folder = Config.train_imgs_path,\n",
        "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
        "    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n",
        "    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n",
        "    transform=train_augments\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=Config.train_bs,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    collate_fn=mixed_collate_imgs_fn\n",
        ")\n",
        "minibatch = next(iter(train_loader))\n",
        "print(minibatch[0].shape)\n",
        "print(minibatch[0].mean((1,2,3)))\n",
        "print(minibatch[0].min())\n",
        "print(minibatch[0].max())\n",
        "minibatch[0] += 1.0\n",
        "minibatch[0] /= 2.0\n",
        "print(minibatch[0].mean((1,2,3)))\n",
        "\n",
        "display_batch(minibatch, maximgs=4)"
      ],
      "metadata": {
        "id": "TyorDvHcervf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_dataset = RSNAData(\n",
        "    df = df,\n",
        "    img_folder = Config.train_imgs_path,\n",
        "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
        "    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n",
        "    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n",
        "    transform=ToTensorV2(),\n",
        "    return_filepath=True\n",
        ")\n",
        "\n",
        "weighted_x_loader = DataLoader(\n",
        "    x_dataset,\n",
        "    batch_size=Config.train_bs,\n",
        "    sampler=torch.utils.data.sampler.WeightedRandomSampler(\n",
        "        class_weight(df[\"cancer\"].values, Config.positive_upsample_to_percent),\n",
        "        len(df),\n",
        "        replacement=False),\n",
        "    num_workers=Config.dataloader_workers_count,\n",
        "    collate_fn=mixed_collate_imgs_fn,\n",
        ")\n",
        "\n",
        "#if Config.debug or True:\n",
        "#    for minibatch in weighted_x_loader:\n",
        "#        imgs = minibatch[0]\n",
        "#        if torch.isnan(minibatch[0]).sum() > 0:\n",
        "#            raise ValueError(\"bad pixel values\")\n",
        "#        if torch.isnan(minibatch[1]).sum() > 0:\n",
        "#            raise ValueError(\"bad targets values\")"
      ],
      "metadata": {
        "id": "jnZYQWyyervf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch = next(iter(weighted_x_loader))\n",
        "display_batch(minibatch, maximgs=10,figsize=(32,32))\n",
        "minibatch[1]"
      ],
      "metadata": {
        "id": "T--jksEGervg",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_dataset = RSNAData(\n",
        "    df = df,\n",
        "    img_folder = Config.train_imgs_path,\n",
        "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
        "    transform = valid_augments\n",
        ")\n",
        "v_loader = DataLoader(\n",
        "    v_dataset,\n",
        "    batch_size=Config.train_bs,\n",
        "    shuffle=False,\n",
        "    num_workers=Config.dataloader_workers_count,\n",
        "    collate_fn=mixed_collate_imgs_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "minibatch = next(iter(v_loader))\n",
        "print(minibatch[0].shape)\n",
        "print(minibatch[0].mean((1,2,3)))\n",
        "print(minibatch[0].min())\n",
        "print(minibatch[0].max())\n",
        "print(torch.isnan(minibatch[0]).sum())\n",
        "minibatch[0] += 1.0\n",
        "minibatch[0] /= 2.0\n",
        "print(minibatch[0].mean((1,2,3)))\n",
        "    \n",
        "display_batch(minibatch, maximgs=20,figsize=(32,32))\n",
        "\n",
        "minibatch[1]\n"
      ],
      "metadata": {
        "id": "bCUFst_Rervh",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=tb_logs/"
      ],
      "metadata": {
        "id": "WPqiKnVQervh",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}