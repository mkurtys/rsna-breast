{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Uncomment this cell to run the notebook with TPUs üëáüèª","metadata":{"id":"Vq4xUH5aervK"}},{"cell_type":"code","source":"# %%capture\n# ! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# ! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","metadata":{"id":"G5H6DRrtervM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5gZOg8Sq89C","outputId":"8debe9b8-b66c-4a93-b573-0caaa7e1f855","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Environment/Dataset configuration\n\n## TLDR\n`kaggle.json` - Kaggle API key\n\nwandb - if enabled enter api key in notebook (need to change - troublesome)\n\n## Details\nBasicaly 3 options:\n* Kaggle environement\n* Google colab environment\n* Other\n\nWhen running on kaggle, kaggle datasets are available since runtime is ready.\nOtherwise one have to download dataset - downloading from Kaggle is good option.\n**In order to download kaggle dataset one need his own API key**\n\n\nOriginal Kaggle DICOM dataset ~300GB needs preprocessing.\n`kaggle competitions download -c rsna-breast-cancer-detection`\n\nCurrently I use community-processed or self-processed training data.\nKaggle supports creating own private datasets up to 20GB each one.","metadata":{}},{"cell_type":"code","source":"import os\nimport os.path\nfrom pathlib import Path\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    IS_KAGGLE = True\nexcept ImportError:\n    IS_KAGGLE = False\n    \nif IS_KAGGLE:\n    DATASET_PATH = \"/kaggle/input/\"\nelse:\n    DATASET_PATH = os.path.expanduser(\"~/rsna-breast\")\n    !mkdir -p {DATASET_PATH}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO add wandb api key\ntry:\n    from google.colab import files\n    if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")):\n        files.upload()\n        #2. Series of commands to set-up for download\n        !ls -lha kaggle.json\n        !mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n        !cp kaggle.json ~/.kaggle/ # move the key to the folder\n        #3. giving rw access (if 401-nathorized)\n        !chmod 600 ~/.kaggle/kaggle.json\nexcept ImportError:\n    pass\nif not IS_KAGGLE:\n    !pip install -q kaggle # installing the kaggle package\n    !mkdir -p ~/kaggle/\n    !kaggle datasets download -d awsaf49/rsna-bcd-roi-1024x512-png-v2-dataset -p {DATASET_PATH}\n    !unzip -q {DATASET_PATH/rsna-bcd-roi-1024x512-png-v2-dataset.zip -d {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset\n    !rm {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150},"id":"kQNAy38OervN","outputId":"7f9d1999-8437-46b2-818b-c17e26a6c863","tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installation and Imports","metadata":{"id":"By3AjokVervP"}},{"cell_type":"code","source":"%%capture\n! pip install --upgrade timm\n! pip install --upgrade wandb\n! pip install --upgrade torchmetrics\n! pip install --upgrade pytorch-lightning\n! pip install --upgrade albumentations\n! pip install --upgrade wandb\n! pip install --upgrade opencv-python","metadata":{"_kg_hide-input":true,"id":"RJTFkspLervP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport sys\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nfrom typing import Iterator, Iterable, Optional, Sequence, List, TypeVar, Generic, Sized, Union\n\nimport wandb\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset, Sampler\n\nimport torchmetrics\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n\nimport sklearn.model_selection\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize, Affine\n)\nimport albumentations.augmentations.transforms\nfrom albumentations.pytorch import ToTensorV2\n\n# import warnings\n# warnings.simplefilter('ignore')\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"TUHr4YJbervQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config File and Wandb","metadata":{"id":"IGp3ez88ervR"}},{"cell_type":"code","source":"class WandbMode:\n    OFFLINE = \"offline\"\n    ONLINE = \"online\"\n    DISABLED = \"disabled\"\n    \nclass WandbConfig:\n    # change to disabled/offline if not needed\n    mode = WandbMode.ONLINE\n    project = \"rsna-breast\"\n    name = None\n    save_dir = None\n    log_model = \"all\"\n    # use specific model artifact to download and start with \n    # TODO add also local model checkpoints!\n    model_artifact_checkpoint_reference = None\n    \n\nclass Config:\n    train_bs=16\n    valid_bs=16\n    model_name=\"tf_efficientnetv2_s\"\n    model_params = {\n      \"drop_rate\": 0.2,\n      \"drop_path_rate\": 0.2\n    }\n    overwrite_checkpoint_hparams=True\n    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_256/train_images_processed_cv2_256\"\n    #train_imgs_path = \"/kaggle/input/rsna-breast-png-roi/train_images/512/\"\n    #train_imgs_path = \"/kaggle/input/rsna-cut-off-empty-space-from-images\"\n    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_vl_512/train_images_processed_cv2_vl_512\"\n    train_imgs_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train_images\"\n    \n    # True if image data folders have structure <patient_id>/<image_id>.png\n    # False if image data have structure <patient_id>_<image_id>.png\n    data_has_patient_folder_sturcture = True\n    \n    #train_csv_path = \"/kaggle/input/rsna-breast-cancer-detection/train.csv\"\n    train_csv_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train.csv\"\n    \n    test_imgs_path = f\"{DATASET_PATH}/rsna-breast-png-roi/test_images/256/\"\n    \n    accelerator=\"auto\"   # auto, gpu or cpu\n    num_devices=1        # TODO - parallel computations\n     \n    use_pretrained=True\n    epochs_count = 20\n    splits_count = 1\n    warmup_lr = 1e-6\n    warmup_epochs = 1\n    lr=5e-3\n    t_max= 8 \n    min_lr= 1e-5\n    weight_decay=1e-5\n    # weight of cancer examples in loss function\n    pos_weight = 1\n    # upsample cancers to percent\n    positive_upsample_to_percent = 0.3\n    # mixed-precision\n    precision=16 if torch.cuda.is_available() else 32 \n    \n    # dataloaders\n    dataloader_workers_count = 4\n    # aux_targets = ['BIRADS'] # not used\n    \n    debug=False\n    debug_data_use_only_percent=0.001\n    \n\ndef config_class_to_dict(config_class):\n    # add recursion if needed\n    return { k:v for k,v in Config.__dict__.items() if not k.startswith(\"__\") }","metadata":{"id":"Jd7ZTkBBervS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Path(Config.train_imgs_path).exists()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSYdv1rKervT","outputId":"f01f1d84-5e1a-45bb-b309-13454087c5e1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = None\n\nwandb_logger = None\nwandb_artifact = None\nwandb_artifact_dir = None\nif WandbConfig.mode != WandbMode.DISABLED:\n    wb_key = None\n    if IS_KAGGLE:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n    # wandb should get key fron environment\n    wandb.login(key=wb_key)\n\n    wandb_logger = WandbLogger(\n        project=WandbConfig.project,\n        name = WandbConfig.name,\n        offline = True if WandbConfig.mode == WandbMode.OFFLINE else False,\n        save_dir = WandbConfig.save_dir,\n        log_model = WandbConfig.log_model,\n        group='vision',\n        job_type='train',\n        config=config_class_to_dict(Config)\n    )\n\n    if WandbConfig.mode == WandbMode.ONLINE and WandbConfig.model_artifact_checkpoint_reference:\n        artifact = run.use_artifact(\n            WandbConfig.model_artifact_checkpoint_reference, type=\"model\")\n        artifact_dir = artifact.download()\n        checkpoint_path = Path(artifact_dir) / \"model.ckpt\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"orXIS0zgervU","outputId":"89fcf1d3-6779-4f8b-de97-f3c31b53fcac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# not used, but efficientnet v2 do not use weight decay\n# for some parameters\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):\n            # print(name, 'no decay')\n            no_decay.append(param)\n        else:\n            # print(name, 'decay')\n            decay.append(param)\n    return [\n        {'params': no_decay, 'weight_decay': 0.},\n        {'params': decay, 'weight_decay': weight_decay}]","metadata":{"id":"liQqkQMCervV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_opt_and_scheduler():\n    opt = torch.optim.SGD([torch.tensor(1)], lr=Config.lr)\n    scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n                                                     factor=Config.warmup_lr/Config.lr,\n                                                     total_iters=20)\n    scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n                opt, \n                T_max=Config.t_max,\n                eta_min=Config.min_lr\n                )\n    scheduler = torch.optim.lr_scheduler.SequentialLR(opt,\n                                                      schedulers=[scheduler1, scheduler2],\n                                                      milestones=[Config.warmup_epochs])\n    return opt, scheduler","metadata":{"id":"rQ3Pmd1HervW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef visualise_scheduler(opt, scheduler, steps=Config.epochs_count, lr=1):\n    lrs = []\n    for _ in range(100):\n        opt.step()\n        lrs.append(scheduler.get_last_lr())\n        scheduler.step()\n    \n    fig, ax = plt.subplots()\n    ax.plot(lrs)\n    fig.show()\nopt, scheduler = create_test_opt_and_scheduler()\nvisualise_scheduler(opt,\n                    scheduler)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"78B6xkRFervW","outputId":"ab8d6b9b-af22-46f5-9cde-f79a4a31827e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def probabilistic_f1(labels, preds, beta=1):\n    \"\"\"\n    Function taken from Awsaf's notebook:\n    https://www.kaggle.com/code/awsaf49/rsna-bcd-efficientnet-tf-tpu-1vm-train\n    \"\"\"\n    eps = 1e-5\n    preds = preds.clip(0, 1)\n    y_true_count = labels.sum()\n    ctp = preds[labels==1].sum()\n    cfp = preds[labels==0].sum()\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp + eps)\n    c_recall = ctp / (y_true_count + eps)\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + eps)\n        return result\n    else:\n        return 0.0","metadata":{"_kg_hide-input":true,"id":"HSo25zKvervX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class - for loading in data","metadata":{"id":"E08Abs2WervY"}},{"cell_type":"code","source":"def display_batch(batch, figsize=(16,10), n_cols=2, maximgs=None, cmap=\"bone\"):\n    if isinstance(batch, tuple) or isinstance(batch, list):\n        if len(batch) == 2:\n            imgs, targets = batch\n        else:\n            imgs, targets, filenames = batch\n    else:\n        imgs = batch\n        targets = None\n    #targets = targets.numpy().squeeze()\n    \n    n = imgs.shape[0] if not maximgs else min(imgs.shape[0], maximgs)\n    n_rows = math.ceil(n / n_cols)\n    \n    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n    for i in range(n):\n        x = i % n_cols\n        y = i // n_cols\n        ax=axs[y,x]\n        im = imgs[i, :, :, :].squeeze()\n        im = im.numpy().transpose((1,2,0)).squeeze()\n        ax.imshow(im, cmap=cmap)\n        if targets is not None:\n            ax.set_title(f'Target {targets[i]}', fontsize=10)\n        ax.set_xticks([]); ax.set_yticks([])\n    fig.tight_layout()\n    fig.show() ","metadata":{"id":"ZW7CKrzzervY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_images_to_max_dim(batch, pad_value = -1):\n    \"Pad images with values if there are images of different size in batch\"\n    # consider non-symmetrical padding\n    \n    # print([img.shape for img in batch])\n    max_h = max([img.shape[-2] for img in batch])\n    max_w = max([img.shape[-1] for img in batch])\n    # pad all images in batch\n    padded_batch = []\n    for i, img in enumerate(batch):\n        pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0\n        diff_w = max_w - img.shape[-1]\n        diff_h = max_h - img.shape[-2]\n        if diff_w > 0:\n            pad_left = diff_w//2\n            pad_right = diff_w - pad_left\n        if diff_h > 0:\n            pad_top = diff_h//2\n            pad_bottom = diff_h - pad_top\n        if any([pad_left, pad_right, pad_top, pad_bottom]):\n            padded_img = torch.nn.functional.pad(img,\n                            (pad_left, pad_right, pad_top, pad_bottom),\n                            value=pad_value)\n            padded_batch.append(padded_img)\n        else:\n            padded_batch.append(img)\n        \n    return padded_batch\n\ndef mixed_collate_imgs_fn(batch):\n    batch_soft_cpy = [x for x in batch]\n    batch_imgs = [x[0] for x in batch]\n    \n    padded_batch = []\n    padded_batch_imgs = pad_images_to_max_dim(batch_imgs)\n    for b_item, pad_img in zip(batch, padded_batch_imgs):\n        padded_batch.append( (pad_img, *b_item[1:]))\n        \n    return torch.utils.data.default_collate(padded_batch)\n\n# supports both 3-channels PNGs, and single-channel 16 bit PNG\nclass RSNAData(Dataset):\n    def __init__(self, df,\n                 img_folder,\n                 resize_dim=None,\n                 transform=None,\n                 is_test=False,\n                 has_patient_folder_sturcture=False,\n                 extension=\"png\",\n                 return_filepath=False):\n        self.df = df\n        self.is_test = is_test\n        self.transform = transform\n        self.img_folder = img_folder\n        self.resize_dim = resize_dim\n        self.extension = extension\n        self.has_patient_folder_sturcture=has_patient_folder_sturcture\n        self.return_filepath=return_filepath\n        \n    def __getitem__(self, idx):\n        row = df.loc[idx, :]\n        \n        if self.has_patient_folder_sturcture:\n            img_path = os.path.join(self.img_folder, str(row[\"patient_id\"]), f\"{row['image_id']}.{self.extension}\")\n        else:\n            img_path = os.path.join(self.img_folder, f\"{row['image_id']}.{self.extension}\")\n        \n        # need anydepth to load 16bit grayscale png\n        # don't know if pngs are bgr or rgb\n        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n        if img is None:\n            raise ValueError(f\"No image {img_path} found\")\n        # cast to np float, as torch can't into uint16\n        if self.resize_dim:\n            img = cv2.resize(img, resize_dim)\n            \n        # convert to RGB for pretrained networks\n        # maybe we will move it to the different place\n        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[0] == 1):\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # should we really normalize here? Why not in transforms?\n        # again there are problems (there are?) with 16bits PNGS\n        img = img.astype(np.float32)\n        img_max = np.amax(img)\n        img_min = np.amin(img)\n        img_range = img_max-img_min\n        if img_range > 0:\n            img = (img-img_min)/img_range\n     \n        if self.transform:\n            img = self.transform(image=img)['image']      \n        \n        # shouldn't be done in transforms?\n        img = torch.tensor(img, dtype=torch.float)\n        \n        if not self.is_test:\n            target = self.df['cancer'][idx]\n            target = torch.tensor(target, dtype=torch.float32)\n            if self.return_filepath:\n                return img, target, img_path\n            return (img, target)\n        if self.return_filepath:\n            return img, target\n        return (img)\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"id":"qumIr-oFervZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test pading\ny= pad_images_to_max_dim([torch.tensor(np.random.rand(256,256)),\n                          torch.tensor(np.random.randn(100,50))])\n[x.shape for x in y]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqX9dNsDerva","outputId":"fdfd0db0-f221-409e-a755-a37df2e2bd20","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data and pass it onto the training function\ndf = pd.read_csv(Config.train_csv_path)\ndf['img_name'] = df['patient_id'].astype(str) + \"/\" + df['image_id'].astype(str) + \".png\"\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":618},"id":"qHIlDM9yerva","outputId":"bd94cb54-2d72-41e9-b665-41c76b553e8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic Image Augmentations","metadata":{"id":"eRlsr6nZerva"}},{"cell_type":"code","source":"\ntrain_augments = Compose([\n    HorizontalFlip(p=0.5),\n    VerticalFlip(p=0.5),\n    Affine(rotate=(-10,10), p=0.2),\n    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n    ToTensorV2(p=1.0)\n],p=1.)\n    \nvalid_augments = Compose([\n    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n    ToTensorV2(p=1.0)\n], p=1.)","metadata":{"id":"RjJyqfU2ervb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO - have exhaustive sampler with upsampling positive examples\n# in contrast to random sampler which is not necessary exhaustive\n# of course maybe we do not need exhaustiveness\nclass PositiveNegativeIndicesSampler(Sampler[int]):\n\n    def __init__(self, positive_indices, negative_indices, num_samples,\n                 negatives_per_batch, batch_size,\n                 generator=None) -> None:\n        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n                num_samples <= 0:\n            raise ValueError(\"num_samples should be a positive integer \"\n                             \"value, but got num_samples={}\".format(num_samples))\n\n        self.positive_indices = positive_indices\n        self.negative_indices = negative_indices\n        self.num_samples = num_samples\n        self.generator = generator\n\n    def __iter__(self) -> Iterator[int]:\n        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n        yield from iter(rand_tensor.tolist())\n\n    def __len__(self) -> int:\n        return self.num_samples","metadata":{"id":"x140jOLrervb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO refactor variable naming\ndef class_weight(class_array, upsample_cancer_to_percent):\n    \"Returns weights for array of target classes so that positive examples are upsampled to given percent\"\n    \n    has_cancer = class_array.astype(np.float64)\n    \n    dataset_len = len(has_cancer)\n    cancer_len = len(has_cancer[has_cancer > 0])\n    non_cancer_len = dataset_len-cancer_len\n    \n    x = non_cancer_len*upsample_cancer_to_percent / (cancer_len -  upsample_cancer_to_percent*cancer_len)\n    \n    has_cancer[has_cancer > 0] = x\n    has_cancer[has_cancer <= 0 ] = 1\n    return has_cancer","metadata":{"id":"PDdEi7-Fervb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Class using `pl.LightningModule` ‚ö°Ô∏è","metadata":{"id":"f1iFj-n_ervc"}},{"cell_type":"code","source":"# test\nclass_weight( np.concatenate((np.zeros(9),np.ones(1))),  0.5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuMcJqYJervc","outputId":"45ea3f74-e09a-4910-898c-49f65960e277","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RSNAModel(pl.LightningModule):\n    def __init__(self, base_model,\n                 lr,\n                 warmup_lr,\n                 warmup_epochs,\n                 t_max,\n                 min_lr,\n                 weight_decay,\n                 features_size=None,\n                 pos_weight=None,   \n                ):\n        super().__init__()\n        # save_hyperparameters() is used to specify which init arguments should \n        # be saved in the checkpoint file to be used to instantiate the model\n        # from the checkpoint later.\n        self.save_hyperparameters(ignore=[\"base_model\", \"features_size\"])\n        # Model Architecture\n        self.model = base_model\n        classes_count = 1\n        if not features_size:\n            features_size = base_model(torch.randn(1, 3, 512, 512)).shape[-1]\n        self.fc = nn.Linear(features_size, classes_count)\n        \n        # Loss functions\n        # CHECKME - I believe that positive weight could not work here after re-loading model\n        pos_weight=None if pos_weight is None else torch.tensor([pos_weight], dtype=torch.float32)\n        self.train_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n        self.valid_loss = nn.BCEWithLogitsLoss()\n\n        # Metric\n        self.roc_auc = torchmetrics.classification.BinaryAUROC()\n        self.pr_curve = torchmetrics.classification.BinaryPrecisionRecallCurve()\n        self.valid_stat_scores = torchmetrics.classification.BinaryStatScores()\n        self.train_stat_scores = torchmetrics.classification.BinaryStatScores()\n        self.binary_f1 = torchmetrics.F1Score(task='binary')\n    \n    def configure_optimizers(self):\n        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n                               weight_decay=self.hparams.weight_decay)\n\n        scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n                                                         factor=self.hparams.warmup_lr/self.hparams.lr,\n                                                         total_iters=self.hparams.warmup_epochs)\n        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n                        opt, \n                        T_max=self.hparams.t_max,\n                        eta_min=self.hparams.min_lr\n                     )\n        sch = torch.optim.lr_scheduler.SequentialLR(opt,\n                                                    schedulers=[scheduler1, scheduler2],\n                                                    milestones=[self.hparams.warmup_epochs])\n        \n        return [opt], [sch]\n        \n    def forward(self, x):\n        features = self.model(x)\n        output = self.fc(features)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        imgs = batch[0]\n        target = batch[1].unsqueeze(-1)\n        \n        out = self(imgs)\n        positive_outcomes_mean = out[out>=0].mean()\n        negative_outcomes_mean = out[out<0].mean()\n        cancer_percent = target[target>0].shape[0]/target.shape[0]\n        \n        predictions = F.sigmoid(out)\n        \n        # shall we compute every x epochs? (but we will loose epoch metrics!)\n        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores(predictions, target)\n        train_loss = self.train_loss(out, target)\n        to_log = {'train/loss': train_loss.item(),\n                  'train/positive_outcomes_mean': positive_outcomes_mean.item(),\n                  'train/negative_outcomes_mean': negative_outcomes_mean.item(),\n                  'train/accuracy': ((true_positives+true_negatives)/(true_positives+false_positives+true_negatives+false_negatives)).item(),\n                  'train/precision': (true_positives/(true_positives+false_positives)).item(),\n                  'train/recall': (true_positives/(true_positives+false_negatives)).item()\n                 }\n        self.log_dict(to_log)\n        return train_loss\n    \n    def validation_step(self, batch, batch_idx):\n        imgs = batch[0]\n        target = batch[1].unsqueeze(-1)\n        \n        out = self(imgs)\n        \n        valid_loss = self.valid_loss(out, target)\n        \n        predictions = F.sigmoid(out)\n          \n        f1_current = self.binary_f1(predictions, target)\n        self.roc_auc(predictions, target)\n        self.valid_stat_scores(predictions, target)\n        self.log_dict({\"valid/loss\": valid_loss.item(),\n                       \"valid/f1\": f1_current.item()})     \n        return valid_loss\n    \n    def training_epoch_end(self, outputs):\n        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores.compute()\n        to_log = {\n                  'train/precision_epoch': (true_positives/(true_positives+false_negatives)).item(),\n                  'train/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n                  'train/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n        self.log_dict(to_log) \n        self.train_stat_scores.reset()\n    \n    def validation_epoch_end(self, outputs):\n        true_positives, false_positives, true_negatives, false_negatives, sup = self.valid_stat_scores.compute()\n        to_log = {'valid/roc_auc_epoch': self.roc_auc.compute().item(),\n                  'valid/f1_epoch': self.binary_f1.compute().item(),\n                  'valid/precision_epoch': (true_positives/(true_positives+false_positives)).item(),\n                  'valid/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n                  'valid/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n        self.log_dict(to_log) \n        self.binary_f1.reset()\n        self.roc_auc.reset()\n        self.valid_stat_scores.reset()\n        \n    # def test_step(self, batch, batch_idx):\n    #    loss, acc = self._shared_eval_step(batch, batch_idx)\n    #    metrics = {\"test_acc\": acc, \"test_loss\": loss}\n    #    self.log_dict(metrics)\n    #    return metrics\n    \n\n\nconfig_hparams_dict = dict(\n    lr=Config.lr,\n    warmup_lr=Config.warmup_lr,\n    warmup_epochs=Config.warmup_epochs,\n    t_max=Config.t_max,\n    min_lr=Config.min_lr,\n    weight_decay=Config.weight_decay,\n    pos_weight=Config.pos_weight)","metadata":{"id":"ewQLdSfdervd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tb_logger = TensorBoardLogger(save_dir=\"tb_logs\", name=\"rsna-breast-model\")\nloggers = [x for x in [wandb_logger, tb_logger] if x is not None]\n\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints\",\n    monitor=\"valid/f1_epoch\",\n    filename=\"ckpt_epoch={epoch:02d}_validf1={valid/f1_epoch:02.0f}\",\n    auto_insert_metric_name=False,\n    save_top_k=3,\n    mode=\"max\",\n    save_last=True,\n    every_n_epochs=3)","metadata":{"id":"SCHzzGNTervd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main loop","metadata":{"id":"imsqGEdPerve"}},{"cell_type":"code","source":"\n\n# add stratified k folds when ready\nkfold = sklearn.model_selection.StratifiedShuffleSplit(\n    n_splits=Config.splits_count, train_size=0.8)\nfor fold_, (train_idx, valid_idx) in enumerate(kfold.split(df, df['cancer'].values)):\n    print(f\"{'='*40} Fold: {fold_} / {Config.splits_count-1} {'='*40}\")\n\n\n    train_df = df.loc[train_idx].reset_index(drop=True).copy()\n    valid_df = df.loc[valid_idx].reset_index(drop=True).copy()\n        \n    if Config.debug:\n        train_df = train_df.iloc[:int(len(train_df)*Config.debug_data_use_only_percent), :]\n        valid_df = train_df.iloc[:int(len(valid_df)*Config.debug_data_use_only_percent), :]\n    \n    train_rows_weight = class_weight(train_df[\"cancer\"].values, Config.positive_upsample_to_percent)   \n\n    print(f\"Train df len {len(train_df)}, cancers {train_df['cancer'].sum()}, percent {train_df['cancer'].sum()/len(train_df)}\")\n    print(f\"Valid df len {len(valid_df)}, cancers {valid_df['cancer'].sum()}, percent {valid_df['cancer'].sum()/len(valid_df)}\")        \n    print(f\"In train dataset cancer will be upsampled to percent {train_rows_weight[train_rows_weight>1].sum()/train_rows_weight.sum()}\")\n    \n    train_dataset = RSNAData(\n        df = train_df,\n        img_folder = Config.train_imgs_path,\n        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n        transform=train_augments\n    )\n    valid_dataset = RSNAData(\n        df = valid_df,\n        img_folder = Config.train_imgs_path,\n        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n        transform = valid_augments\n    )\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=Config.train_bs,\n        sampler=torch.utils.data.sampler.WeightedRandomSampler(train_rows_weight, len(train_df)),\n        num_workers=Config.dataloader_workers_count,\n        collate_fn=mixed_collate_imgs_fn,\n        pin_memory=True\n    )\n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=Config.train_bs,\n        shuffle=False,\n        num_workers=Config.dataloader_workers_count,\n        collate_fn=mixed_collate_imgs_fn,\n        pin_memory=True\n    )\n    # CHECKME after introducing k-fold start with fresh copy of base model\n    \n    if checkpoint_path:\n        if Config.overwrite_checkpoint_hparams:\n            hparams = config_hparams_dict\n        else:\n            hparams = {}\n        model = RSNAModel.load_from_checkpoint(checkpoint_path, \n                                              **hparams)\n    else:   \n        # it turns that i have lower LB than other kagglers using efficientnet.\n        # i have forgotten to set the drop path rate. Here is the fixed:\n        # efficientnet_b2(pretrained=True, drop_rate = 0.3, drop_path_rate = 0.2)\n        model_name = Config.model_name\n        base_model = timm.create_model(model_name, \n                                       pretrained=Config.use_pretrained,\n                                       **Config.model_params\n                                      ) \n        model = RSNAModel(base_model,\n                          **config_hparams_dict)\n    \n    trainer = pl.Trainer(\n        max_epochs=Config.epochs_count,\n        accelerator=Config.accelerator,\n        devices=Config.num_devices,\n        precision=Config.precision,\n        logger = loggers,\n        callbacks=[checkpoint_callback]\n    )\n    trainer.fit(model, train_loader, valid_loader)\n    wandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAEMsNwmerve","outputId":"662d5026-5ce4-448c-f6bc-b71ea566e366","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = RSNAData(\n    df = df,\n    img_folder = Config.train_imgs_path,\n    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n    transform=train_augments\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=Config.train_bs,\n    shuffle=True,\n    num_workers=1,\n    collate_fn=mixed_collate_imgs_fn\n)\nminibatch = next(iter(train_loader))\nprint(minibatch[0].shape)\nprint(minibatch[0].mean((1,2,3)))\nprint(minibatch[0].min())\nprint(minibatch[0].max())\nminibatch[0] += 1.0\nminibatch[0] /= 2.0\nprint(minibatch[0].mean((1,2,3)))\n\ndisplay_batch(minibatch, maximgs=4)","metadata":{"id":"TyorDvHcervf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_dataset = RSNAData(\n    df = df,\n    img_folder = Config.train_imgs_path,\n    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n    transform=ToTensorV2(),\n    return_filepath=True\n)\n\nweighted_x_loader = DataLoader(\n    x_dataset,\n    batch_size=Config.train_bs,\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(\n        class_weight(df[\"cancer\"].values, Config.positive_upsample_to_percent),\n        len(df),\n        replacement=False),\n    num_workers=Config.dataloader_workers_count,\n    collate_fn=mixed_collate_imgs_fn,\n)\n\n#if Config.debug or True:\n#    for minibatch in weighted_x_loader:\n#        imgs = minibatch[0]\n#        if torch.isnan(minibatch[0]).sum() > 0:\n#            raise ValueError(\"bad pixel values\")\n#        if torch.isnan(minibatch[1]).sum() > 0:\n#            raise ValueError(\"bad targets values\")","metadata":{"id":"jnZYQWyyervf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"minibatch = next(iter(weighted_x_loader))\ndisplay_batch(minibatch, maximgs=10,figsize=(32,32))\nminibatch[1]","metadata":{"id":"T--jksEGervg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v_dataset = RSNAData(\n    df = df,\n    img_folder = Config.train_imgs_path,\n    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n    transform = valid_augments\n)\nv_loader = DataLoader(\n    v_dataset,\n    batch_size=Config.train_bs,\n    shuffle=False,\n    num_workers=Config.dataloader_workers_count,\n    collate_fn=mixed_collate_imgs_fn,\n    pin_memory=True\n)\nminibatch = next(iter(v_loader))\nprint(minibatch[0].shape)\nprint(minibatch[0].mean((1,2,3)))\nprint(minibatch[0].min())\nprint(minibatch[0].max())\nprint(torch.isnan(minibatch[0]).sum())\nminibatch[0] += 1.0\nminibatch[0] /= 2.0\nprint(minibatch[0].mean((1,2,3)))\n    \ndisplay_batch(minibatch, maximgs=20,figsize=(32,32))\n\nminibatch[1]\n","metadata":{"id":"bCUFst_Rervh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reload_ext tensorboard\n%tensorboard --logdir=tb_logs/","metadata":{"id":"WPqiKnVQervh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Lrq5pcgServh"},"execution_count":null,"outputs":[]}]}