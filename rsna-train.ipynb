{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq4xUH5aervK"
   },
   "source": [
    "Uncomment this cell to run the notebook with TPUs üëáüèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G5H6DRrtervM"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# ! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5gZOg8Sq89C",
    "outputId": "344d2948-ad44-47fb-9133-f6da4f95282b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 14 11:21:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                 ERR! |\n",
      "| 26%   34C    P8     6W / 180W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "WARNING: infoROM is corrupted at gpu 0000:00:05.0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "885GuIVA6XCJ"
   },
   "source": [
    "### Environment/Dataset configuration\n",
    "\n",
    "## TLDR\n",
    "`kaggle.json` - Kaggle API key\n",
    "\n",
    "wandb - if enabled enter api key in notebook (need to change - troublesome)\n",
    "\n",
    "## Details\n",
    "Basicaly 3 options:\n",
    "* Kaggle environement\n",
    "* Google colab environment\n",
    "* Other\n",
    "\n",
    "When running on kaggle, kaggle datasets are available since runtime is ready.\n",
    "Otherwise one have to download dataset - downloading from Kaggle is good option.\n",
    "**In order to download kaggle dataset one need his own API key**\n",
    "\n",
    "\n",
    "Original Kaggle DICOM dataset ~300GB needs preprocessing.\n",
    "`kaggle competitions download -c rsna-breast-cancer-detection`\n",
    "\n",
    "Currently I use community-processed or self-processed training data.\n",
    "Kaggle supports creating own private datasets up to 20GB each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7EyqMVYw6XCK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    IS_KAGGLE = True\n",
    "except ImportError:\n",
    "    IS_KAGGLE = False\n",
    "    \n",
    "if IS_KAGGLE:\n",
    "    DATASET_PATH = \"/kaggle/input/\"\n",
    "else:\n",
    "    DATASET_PATH = os.path.expanduser(\"~/rsna-breast\")\n",
    "    !mkdir -p {DATASET_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "kQNAy38OervN",
    "outputId": "912c6883-a24d-4e59-a7af-3773722fe3dc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading rsna-bcd-roi-1024x512-png-v2-dataset.zip to /root/rsna-breast\n",
      "  1%|‚ñè                                     | 62.0M/11.1G [00:05<22:40, 8.70MB/s]^C\n",
      "  1%|‚ñè                                     | 62.0M/11.1G [00:05<17:40, 11.2MB/s]\n",
      "User cancelled operation\n",
      "[/root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset.zip]\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of /root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset.zip or\n",
      "        /root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset.zip.zip, and cannot find /root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# TODO add wandb api key\n",
    "try:\n",
    "    from google.colab import files\n",
    "    if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")):\n",
    "        files.upload()\n",
    "        #2. Series of commands to set-up for download\n",
    "        !ls -lha kaggle.json\n",
    "        !mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n",
    "        !cp kaggle.json ~/.kaggle/ # move the key to the folder\n",
    "        #3. giving rw access (if 401-nathorized)\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "except ImportError:\n",
    "    pass\n",
    "if not IS_KAGGLE:\n",
    "    !pip install -q kaggle # installing the kaggle package\n",
    "    if not os.path.exists(os.path.expanduser(\"~/.kaggle/kaggle.json\")) and os.path.exists(\"kaggle.json\"):\n",
    "        !mkdir -p ~/kaggle/\n",
    "        !cp kaggle.json ~/.kaggle/ # move the key to the folder\n",
    "        #3. giving rw access (if 401-nathorized)\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    !kaggle datasets download -d awsaf49/rsna-bcd-roi-1024x512-png-v2-dataset -p {DATASET_PATH}\n",
    "    !unzip -q {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset.zip -d {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset\n",
    "    !rm {DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By3AjokVervP"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "id": "RJTFkspLervP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install --upgrade timm\n",
    "! pip install --upgrade wandb\n",
    "! pip install --upgrade torchmetrics\n",
    "! pip install --upgrade pytorch-lightning\n",
    "! pip install --upgrade albumentations\n",
    "! pip install --upgrade wandb\n",
    "! pip install --upgrade opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "TUHr4YJbervQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import enum\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterator, Iterable, Optional, Sequence, List, TypeVar, Generic, Sized, Union\n",
    "\n",
    "import wandb\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize, Affine\n",
    ")\n",
    "import albumentations.augmentations.transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGp3ez88ervR"
   },
   "source": [
    "## Config File and Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jd7ZTkBBervS"
   },
   "outputs": [],
   "source": [
    "class WandbMode:\n",
    "    OFFLINE = \"offline\"\n",
    "    ONLINE = \"online\"\n",
    "    DISABLED = \"disabled\"\n",
    "    \n",
    "class WandbConfig:\n",
    "    # change to disabled/offline if not needed\n",
    "    mode = WandbMode.ONLINE\n",
    "    project = \"rsna-breast\"\n",
    "    name = None\n",
    "    save_dir = None\n",
    "    log_model = \"all\"\n",
    "\n",
    "class ModelCheckpointSource(str, enum.Enum):\n",
    "    NO_CHECKPOINT=\"no_checkpoint\"\n",
    "    WANDB=\"wandb\"\n",
    "    LOCAL=\"local\"\n",
    "\n",
    "class Config:\n",
    "    train_bs=16\n",
    "    valid_bs=16\n",
    "    model_name=\"tf_efficientnetv2_s\"\n",
    "    model_params = {\n",
    "      \"drop_rate\": 0.2,\n",
    "      \"drop_path_rate\": 0.2\n",
    "    }\n",
    "    \n",
    "    model_checkpoint_source = ModelCheckpointSource.LOCAL\n",
    "    model_checkpoint_uri = \"checkpoints/last-v1.ckpt\"\n",
    "    # True, to use Config params instead of checkpoints\n",
    "    overwrite_checkpoint_hparams=True\n",
    "    \n",
    "    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_256/train_images_processed_cv2_256\"\n",
    "    #train_imgs_path = \"/kaggle/input/rsna-breast-png-roi/train_images/512/\"\n",
    "    #train_imgs_path = \"/kaggle/input/rsna-cut-off-empty-space-from-images\"\n",
    "    #train_imgs_path = \"/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_vl_512/train_images_processed_cv2_vl_512\"\n",
    "    train_imgs_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train_images\"\n",
    "    \n",
    "    # True if image data folders have structure <patient_id>/<image_id>.png\n",
    "    # False if image data have structure <patient_id>_<image_id>.png\n",
    "    data_has_patient_folder_sturcture = True\n",
    "    \n",
    "    #train_csv_path = \"/kaggle/input/rsna-breast-cancer-detection/train.csv\"\n",
    "    train_csv_path = f\"{DATASET_PATH}/rsna-bcd-roi-1024x512-png-v2-dataset/train.csv\"\n",
    "    \n",
    "    test_imgs_path = f\"{DATASET_PATH}/rsna-breast-png-roi/test_images/256/\"\n",
    "    \n",
    "    accelerator=\"auto\"   # auto, gpu or cpu\n",
    "    num_devices=1        # TODO - parallel computations\n",
    "\n",
    "    resize_aspect_ratio = 1/2\n",
    "     \n",
    "    use_pretrained=True\n",
    "    epochs_count = 20\n",
    "    splits_count = 1\n",
    "    warmup_lr = 1e-6\n",
    "    warmup_epochs = 1\n",
    "    lr=5e-3\n",
    "    t_max= 8 \n",
    "    min_lr= 1e-5\n",
    "    weight_decay=1e-5\n",
    "    # weight of cancer examples in loss function\n",
    "    pos_weight = 1\n",
    "    # upsample cancers to percent\n",
    "    positive_upsample_to_percent = 0.3\n",
    "    # mixed-precision\n",
    "    precision=16 if torch.cuda.is_available() else 32 \n",
    "    \n",
    "    # dataloaders\n",
    "    dataloader_workers_count = 4\n",
    "    # aux_targets = ['BIRADS'] # not used\n",
    "    \n",
    "    debug=False\n",
    "    debug_data_use_only_percent=0.001\n",
    "    \n",
    "\n",
    "def config_class_to_dict(config_class):\n",
    "    # add recursion if needed\n",
    "    return { k:v for k,v in Config.__dict__.items() if not k.startswith(\"__\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSYdv1rKervT",
    "outputId": "743b73db-35c6-42ab-e889-43ed952ab58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/rsna-breast/rsna-bcd-roi-1024x512-png-v2-dataset/train_images\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(Config.train_imgs_path)\n",
    "print(Path(Config.train_imgs_path).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orXIS0zgervU",
    "outputId": "aafdf05f-5b1a-426c-e6d0-81007655b1ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichal-kurtys\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/rsna-breast/wandb/run-20230114_112430-2k3pin0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/michal-kurtys/rsna-breast/runs/2k3pin0j\" target=\"_blank\">swift-frost-74</a></strong> to <a href=\"https://wandb.ai/michal-kurtys/rsna-breast\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/michal-kurtys/rsna-breast\" target=\"_blank\">https://wandb.ai/michal-kurtys/rsna-breast</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/michal-kurtys/rsna-breast/runs/2k3pin0j\" target=\"_blank\">https://wandb.ai/michal-kurtys/rsna-breast/runs/2k3pin0j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = None\n",
    "wandb_artifact = None\n",
    "wandb_artifact_dir = None\n",
    "if WandbConfig.mode != WandbMode.DISABLED:\n",
    "    wb_key = None\n",
    "    if IS_KAGGLE:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    else:\n",
    "        wb_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "    # wandb should get key fron environment\n",
    "    wandb.login(key=wb_key)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=WandbConfig.project,\n",
    "        name = WandbConfig.name,\n",
    "        offline = True if WandbConfig.mode == WandbMode.OFFLINE else False,\n",
    "        save_dir = WandbConfig.save_dir,\n",
    "        log_model = WandbConfig.log_model,\n",
    "        group='vision',\n",
    "        job_type='train',\n",
    "        config=config_class_to_dict(Config)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = None\n",
    "if Config.model_checkpoint_source==ModelCheckpointSource.LOCAL:\n",
    "    checkpoint_path = Path(Config.model_checkpoint_uri)\n",
    "if WandbConfig.mode == WandbMode.ONLINE and Config.model_checkpoint_source==ModelCheckpointSource.WANDB:\n",
    "    artifact = wandb.run.use_artifact(\n",
    "        Config.model_checkpoint_uri, type=\"model\")\n",
    "    artifact_dir = artifact.download()\n",
    "    checkpoint_path = Path(artifact_dir) / \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "liQqkQMCervV"
   },
   "outputs": [],
   "source": [
    "# not used, but efficientnet v2 do not use weight decay\n",
    "# for some parameters\n",
    "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if len(param.shape) == 1 or np.any([v in name.lower()  for v in skip_list]):\n",
    "            # print(name, 'no decay')\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            # print(name, 'decay')\n",
    "            decay.append(param)\n",
    "    return [\n",
    "        {'params': no_decay, 'weight_decay': 0.},\n",
    "        {'params': decay, 'weight_decay': weight_decay}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rQ3Pmd1HervW"
   },
   "outputs": [],
   "source": [
    "def create_test_opt_and_scheduler():\n",
    "    opt = torch.optim.SGD([torch.tensor(1)], lr=Config.lr)\n",
    "    scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n",
    "                                                     factor=Config.warmup_lr/Config.lr,\n",
    "                                                     total_iters=20)\n",
    "    scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                opt, \n",
    "                T_max=Config.t_max,\n",
    "                eta_min=Config.min_lr\n",
    "                )\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(opt,\n",
    "                                                      schedulers=[scheduler1, scheduler2],\n",
    "                                                      milestones=[Config.warmup_epochs])\n",
    "    return opt, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "78B6xkRFervW",
    "outputId": "20a9e386-d3ca-43ea-dee1-a1840280ea15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def visualise_scheduler(opt, scheduler, steps=Config.epochs_count, lr=1):\n",
    "    lrs = []\n",
    "    for _ in range(100):\n",
    "        opt.step()\n",
    "        lrs.append(scheduler.get_last_lr())\n",
    "        scheduler.step()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(lrs)\n",
    "    fig.show()\n",
    "opt, scheduler = create_test_opt_and_scheduler()\n",
    "visualise_scheduler(opt,\n",
    "                    scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true,
    "id": "HSo25zKvervX"
   },
   "outputs": [],
   "source": [
    "def probabilistic_f1(labels, preds, beta=1):\n",
    "    \"\"\"\n",
    "    Function taken from Awsaf's notebook:\n",
    "    https://www.kaggle.com/code/awsaf49/rsna-bcd-efficientnet-tf-tpu-1vm-train\n",
    "    \"\"\"\n",
    "    eps = 1e-5\n",
    "    preds = preds.clip(0, 1)\n",
    "    y_true_count = labels.sum()\n",
    "    ctp = preds[labels==1].sum()\n",
    "    cfp = preds[labels==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "    c_precision = ctp / (ctp + cfp + eps)\n",
    "    c_recall = ctp / (y_true_count + eps)\n",
    "    if (c_precision > 0 and c_recall > 0):\n",
    "        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + eps)\n",
    "        return result\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E08Abs2WervY"
   },
   "source": [
    "## Dataset Class - for loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZW7CKrzzervY"
   },
   "outputs": [],
   "source": [
    "def display_batch(batch, figsize=(16,10), n_cols=2, maximgs=None, cmap=\"bone\"):\n",
    "    if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "        if len(batch) == 2:\n",
    "            imgs, targets = batch\n",
    "        else:\n",
    "            imgs, targets, filenames = batch\n",
    "    else:\n",
    "        imgs = batch\n",
    "        targets = None\n",
    "    #targets = targets.numpy().squeeze()\n",
    "    \n",
    "    n = imgs.shape[0] if not maximgs else min(imgs.shape[0], maximgs)\n",
    "    n_rows = math.ceil(n / n_cols)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=figsize)\n",
    "    for i in range(n):\n",
    "        x = i % n_cols\n",
    "        y = i // n_cols\n",
    "        ax=axs[y,x]\n",
    "        im = imgs[i, :, :, :].squeeze()\n",
    "        im = im.numpy().transpose((1,2,0)).squeeze()\n",
    "        ax.imshow(im, cmap=cmap)\n",
    "        if targets is not None:\n",
    "            ax.set_title(f'Target {targets[i]}', fontsize=10)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qumIr-oFervZ"
   },
   "outputs": [],
   "source": [
    "def pad_images_to_max_dim(batch, pad_value = -1):\n",
    "    \"Pad images with values if there are images of different size in batch\"\n",
    "    # consider non-symmetrical padding\n",
    "    \n",
    "    # print([img.shape for img in batch])\n",
    "    max_h = max([img.shape[-2] for img in batch])\n",
    "    max_w = max([img.shape[-1] for img in batch])\n",
    "    # pad all images in batch\n",
    "    padded_batch = []\n",
    "    for i, img in enumerate(batch):\n",
    "        pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0\n",
    "        diff_w = max_w - img.shape[-1]\n",
    "        diff_h = max_h - img.shape[-2]\n",
    "        if diff_w > 0:\n",
    "            pad_left = diff_w//2\n",
    "            pad_right = diff_w - pad_left\n",
    "        if diff_h > 0:\n",
    "            pad_top = diff_h//2\n",
    "            pad_bottom = diff_h - pad_top\n",
    "        if any([pad_left, pad_right, pad_top, pad_bottom]):\n",
    "            padded_img = torch.nn.functional.pad(img,\n",
    "                            (pad_left, pad_right, pad_top, pad_bottom),\n",
    "                            value=pad_value)\n",
    "            padded_batch.append(padded_img)\n",
    "        else:\n",
    "            padded_batch.append(img)\n",
    "        \n",
    "    return padded_batch\n",
    "\n",
    "def mixed_collate_imgs_fn(batch):\n",
    "    batch_soft_cpy = [x for x in batch]\n",
    "    batch_imgs = [x[0] for x in batch]\n",
    "    \n",
    "    padded_batch = []\n",
    "    padded_batch_imgs = pad_images_to_max_dim(batch_imgs)\n",
    "    for b_item, pad_img in zip(batch, padded_batch_imgs):\n",
    "        padded_batch.append( (pad_img, *b_item[1:]))\n",
    "        \n",
    "    return torch.utils.data.default_collate(padded_batch)\n",
    "\n",
    "# supports both 3-channels PNGs, and single-channel 16 bit PNG\n",
    "class RSNAData(Dataset):\n",
    "    def __init__(self, df,\n",
    "                 img_folder,\n",
    "                 resize_dim=None,\n",
    "                 resize_aspect_ratio=None,\n",
    "                 transform=None,\n",
    "                 is_test=False,\n",
    "                 has_patient_folder_sturcture=False,\n",
    "                 extension=\"png\",\n",
    "                 return_filepath=False):\n",
    "        \n",
    "        assert not (resize_dim and resize_aspect_ratio)\n",
    "        self.df = df\n",
    "        self.is_test = is_test\n",
    "        self.transform = transform\n",
    "        self.img_folder = img_folder\n",
    "        self.resize_dim = resize_dim\n",
    "        self.resize_aspect_ratio=resize_aspect_ratio\n",
    "        self.extension = extension\n",
    "        self.has_patient_folder_sturcture=has_patient_folder_sturcture\n",
    "        self.return_filepath=return_filepath\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = df.loc[idx, :]\n",
    "        \n",
    "        if self.has_patient_folder_sturcture:\n",
    "            img_path = os.path.join(self.img_folder, str(row[\"patient_id\"]), f\"{row['image_id']}.{self.extension}\")\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_folder, f\"{row['image_id']}.{self.extension}\")\n",
    "        \n",
    "        # need anydepth to load 16bit grayscale png\n",
    "        # don't know if pngs are bgr or rgb\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"No image {img_path} found\")\n",
    "        # cast to np float, as torch can't into uint16\n",
    "        if self.resize_dim:\n",
    "            img = cv2.resize(img, self.resize_dim)\n",
    "        if self.resize_aspect_ratio and self.resize_aspect_ratio!=1.0:\n",
    "            img = cv2.resize(img, tuple(int(d*self.resize_aspect_ratio) for d in img.shape[::-1]))\n",
    "                    \n",
    "        # convert to RGB for pretrained networks\n",
    "        # maybe we will move it to the different place\n",
    "        if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[0] == 1):\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # should we really normalize here? Why not in transforms?\n",
    "        # again there are problems (there are?) with 16bits PNGS\n",
    "        img = img.astype(np.float32)\n",
    "        img_max = np.amax(img)\n",
    "        img_min = np.amin(img)\n",
    "        img_range = img_max-img_min\n",
    "        if img_range > 0:\n",
    "            img = (img-img_min)/img_range\n",
    "     \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']      \n",
    "        \n",
    "        # shouldn't be done in transforms?\n",
    "        img = torch.tensor(img, dtype=torch.float)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            target = self.df['cancer'][idx]\n",
    "            target = torch.tensor(target, dtype=torch.float32)\n",
    "            if self.return_filepath:\n",
    "                return img, target, img_path\n",
    "            return (img, target)\n",
    "        if self.return_filepath:\n",
    "            return img, target\n",
    "        return (img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqX9dNsDerva",
    "outputId": "8bc0c9ae-1049-4004-b9d8-29f674197ff1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([256, 256]), torch.Size([256, 256])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test pading\n",
    "y= pad_images_to_max_dim([torch.tensor(np.random.rand(256,256)),\n",
    "                          torch.tensor(np.random.randn(100,50))])\n",
    "[x.shape for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "qHIlDM9yerva",
    "outputId": "895d48f3-aed8-4a0f-9c1a-4627755ed7f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>laterality</th>\n",
       "      <th>view</th>\n",
       "      <th>age</th>\n",
       "      <th>cancer</th>\n",
       "      <th>biopsy</th>\n",
       "      <th>invasive</th>\n",
       "      <th>BIRADS</th>\n",
       "      <th>implant</th>\n",
       "      <th>density</th>\n",
       "      <th>machine_id</th>\n",
       "      <th>difficult_negative_case</th>\n",
       "      <th>image_path</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>img_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>10050</td>\n",
       "      <td>1428987847</td>\n",
       "      <td>R</td>\n",
       "      <td>MLO</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmp/dataset/part0/train_images/10050/14289878...</td>\n",
       "      <td>410</td>\n",
       "      <td>883</td>\n",
       "      <td>10050/1428987847.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37722</td>\n",
       "      <td>244433205</td>\n",
       "      <td>R</td>\n",
       "      <td>MLO</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmp/dataset/part2/train_images/37722/24443320...</td>\n",
       "      <td>570</td>\n",
       "      <td>1017</td>\n",
       "      <td>37722/244433205.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3930</td>\n",
       "      <td>416807762</td>\n",
       "      <td>R</td>\n",
       "      <td>CC</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmp/dataset/part2/train_images/3930/416807762...</td>\n",
       "      <td>598</td>\n",
       "      <td>982</td>\n",
       "      <td>3930/416807762.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>59312</td>\n",
       "      <td>1962180748</td>\n",
       "      <td>R</td>\n",
       "      <td>CC</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>49</td>\n",
       "      <td>True</td>\n",
       "      <td>/tmp/dataset/part4/train_images/59312/19621807...</td>\n",
       "      <td>341</td>\n",
       "      <td>815</td>\n",
       "      <td>59312/1962180748.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>64133</td>\n",
       "      <td>1343175923</td>\n",
       "      <td>R</td>\n",
       "      <td>CC</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmp/dataset/part4/train_images/64133/13431759...</td>\n",
       "      <td>476</td>\n",
       "      <td>920</td>\n",
       "      <td>64133/1343175923.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   site_id  patient_id    image_id laterality view   age  cancer  biopsy  \\\n",
       "0        2       10050  1428987847          R  MLO  67.0       0       0   \n",
       "1        1       37722   244433205          R  MLO  72.0       0       0   \n",
       "2        2        3930   416807762          R   CC  70.0       0       0   \n",
       "3        1       59312  1962180748          R   CC  68.0       0       0   \n",
       "4        1       64133  1343175923          R   CC  62.0       0       0   \n",
       "\n",
       "   invasive  BIRADS  implant density  machine_id  difficult_negative_case  \\\n",
       "0         0     NaN        0     NaN          29                    False   \n",
       "1         0     2.0        0       B          49                    False   \n",
       "2         0     NaN        0     NaN          29                    False   \n",
       "3         0     0.0        0       C          49                     True   \n",
       "4         0     1.0        0       A          49                    False   \n",
       "\n",
       "                                          image_path  width  height  \\\n",
       "0  /tmp/dataset/part0/train_images/10050/14289878...    410     883   \n",
       "1  /tmp/dataset/part2/train_images/37722/24443320...    570    1017   \n",
       "2  /tmp/dataset/part2/train_images/3930/416807762...    598     982   \n",
       "3  /tmp/dataset/part4/train_images/59312/19621807...    341     815   \n",
       "4  /tmp/dataset/part4/train_images/64133/13431759...    476     920   \n",
       "\n",
       "               img_name  \n",
       "0  10050/1428987847.png  \n",
       "1   37722/244433205.png  \n",
       "2    3930/416807762.png  \n",
       "3  59312/1962180748.png  \n",
       "4  64133/1343175923.png  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and pass it onto the training function\n",
    "df = pd.read_csv(Config.train_csv_path)\n",
    "df['img_name'] = df['patient_id'].astype(str) + \"/\" + df['image_id'].astype(str) + \".png\"\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRlsr6nZerva"
   },
   "source": [
    "## Basic Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RjJyqfU2ervb"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_augments = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    Affine(rotate=(-10,10), p=0.2),\n",
    "    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n",
    "    ToTensorV2(p=1.0)\n",
    "],p=1.)\n",
    "    \n",
    "valid_augments = Compose([\n",
    "    Normalize (mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), max_pixel_value=1.0, p=1.0),\n",
    "    ToTensorV2(p=1.0)\n",
    "], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "x140jOLrervb"
   },
   "outputs": [],
   "source": [
    "# TODO - have exhaustive sampler with upsampling positive examples\n",
    "# in contrast to random sampler which is not necessary exhaustive\n",
    "# of course maybe we do not need exhaustiveness\n",
    "class PositiveNegativeIndicesSampler(Sampler[int]):\n",
    "\n",
    "    def __init__(self, positive_indices, negative_indices, num_samples,\n",
    "                 negatives_per_batch, batch_size,\n",
    "                 generator=None) -> None:\n",
    "        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n",
    "                num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integer \"\n",
    "                             \"value, but got num_samples={}\".format(num_samples))\n",
    "\n",
    "        self.positive_indices = positive_indices\n",
    "        self.negative_indices = negative_indices\n",
    "        self.num_samples = num_samples\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n",
    "        yield from iter(rand_tensor.tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PDdEi7-Fervb"
   },
   "outputs": [],
   "source": [
    "# TODO refactor variable naming\n",
    "def class_weight(class_array, upsample_cancer_to_percent):\n",
    "    \"Returns weights for array of target classes so that positive examples are upsampled to given percent\"\n",
    "    \n",
    "    has_cancer = class_array.astype(np.float64)\n",
    "    \n",
    "    dataset_len = len(has_cancer)\n",
    "    cancer_len = len(has_cancer[has_cancer > 0])\n",
    "    non_cancer_len = dataset_len-cancer_len\n",
    "    \n",
    "    x = non_cancer_len*upsample_cancer_to_percent / (cancer_len -  upsample_cancer_to_percent*cancer_len)\n",
    "    \n",
    "    has_cancer[has_cancer > 0] = x\n",
    "    has_cancer[has_cancer <= 0 ] = 1\n",
    "    return has_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1iFj-n_ervc"
   },
   "source": [
    "## Model Class using `pl.LightningModule` ‚ö°Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WuMcJqYJervc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 9.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "class_weight( np.concatenate((np.zeros(9),np.ones(1))),  0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ewQLdSfdervd"
   },
   "outputs": [],
   "source": [
    "class RSNAModel(pl.LightningModule):\n",
    "    def __init__(self, base_model,\n",
    "                 lr,\n",
    "                 warmup_lr,\n",
    "                 warmup_epochs,\n",
    "                 t_max,\n",
    "                 min_lr,\n",
    "                 weight_decay,\n",
    "                 features_size=None,\n",
    "                 pos_weight=None,   \n",
    "                ):\n",
    "        super().__init__()\n",
    "        # save_hyperparameters() is used to specify which init arguments should \n",
    "        # be saved in the checkpoint file to be used to instantiate the model\n",
    "        # from the checkpoint later.\n",
    "        self.save_hyperparameters(ignore=[\"base_model\", \"features_size\"])\n",
    "        # Model Architecture\n",
    "        self.model = base_model\n",
    "        classes_count = 1\n",
    "        if not features_size:\n",
    "            features_size = base_model(torch.randn(1, 3, 512, 512)).shape[-1]\n",
    "        self.fc = nn.Linear(features_size, classes_count)\n",
    "        \n",
    "        # Loss functions\n",
    "        # CHECKME - I believe that positive weight could not work here after re-loading model\n",
    "        pos_weight=None if pos_weight is None else torch.tensor([pos_weight], dtype=torch.float32)\n",
    "        self.train_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.valid_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Metric\n",
    "        self.roc_auc = torchmetrics.classification.BinaryAUROC()\n",
    "        self.pr_curve = torchmetrics.classification.BinaryPrecisionRecallCurve()\n",
    "        self.valid_stat_scores = torchmetrics.classification.BinaryStatScores()\n",
    "        self.train_stat_scores = torchmetrics.classification.BinaryStatScores()\n",
    "        self.binary_f1 = torchmetrics.F1Score(task='binary')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.lr,\n",
    "                               weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "        scheduler1 = torch.optim.lr_scheduler.ConstantLR(opt,\n",
    "                                                         factor=self.hparams.warmup_lr/self.hparams.lr,\n",
    "                                                         total_iters=self.hparams.warmup_epochs)\n",
    "        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                        opt, \n",
    "                        T_max=self.hparams.t_max,\n",
    "                        eta_min=self.hparams.min_lr\n",
    "                     )\n",
    "        sch = torch.optim.lr_scheduler.SequentialLR(opt,\n",
    "                                                    schedulers=[scheduler1, scheduler2],\n",
    "                                                    milestones=[self.hparams.warmup_epochs])\n",
    "        \n",
    "        return [opt], [sch]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs = batch[0]\n",
    "        target = batch[1].unsqueeze(-1)\n",
    "        \n",
    "        out = self(imgs)\n",
    "        positive_outcomes_mean = out[out>=0].mean()\n",
    "        negative_outcomes_mean = out[out<0].mean()\n",
    "        cancer_percent = target[target>0].shape[0]/target.shape[0]\n",
    "        \n",
    "        predictions = F.sigmoid(out)\n",
    "        \n",
    "        # shall we compute every x epochs? (but we will loose epoch metrics!)\n",
    "        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores(predictions, target)\n",
    "        train_loss = self.train_loss(out, target)\n",
    "        to_log = {'train/loss': train_loss.item(),\n",
    "                  'train/positive_outcomes_mean': positive_outcomes_mean.item(),\n",
    "                  'train/negative_outcomes_mean': negative_outcomes_mean.item(),\n",
    "                  'train/accuracy': ((true_positives+true_negatives)/(true_positives+false_positives+true_negatives+false_negatives)).item(),\n",
    "                  'train/precision': (true_positives/(true_positives+false_positives)).item(),\n",
    "                  'train/recall': (true_positives/(true_positives+false_negatives)).item()\n",
    "                 }\n",
    "        self.log_dict(to_log)\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs = batch[0]\n",
    "        target = batch[1].unsqueeze(-1)\n",
    "        \n",
    "        out = self(imgs)\n",
    "        \n",
    "        valid_loss = self.valid_loss(out, target)\n",
    "        \n",
    "        predictions = F.sigmoid(out)\n",
    "          \n",
    "        f1_current = self.binary_f1(predictions, target)\n",
    "        self.roc_auc(predictions, target)\n",
    "        self.valid_stat_scores(predictions, target)\n",
    "        self.log_dict({\"valid/loss\": valid_loss.item(),\n",
    "                       \"valid/f1\": f1_current.item()})     \n",
    "        return valid_loss\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        true_positives, false_positives, true_negatives, false_negatives, sup = self.train_stat_scores.compute()\n",
    "        to_log = {\n",
    "                  'train/precision_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
    "                  'train/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
    "                  'train/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n",
    "        self.log_dict(to_log) \n",
    "        self.train_stat_scores.reset()\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        true_positives, false_positives, true_negatives, false_negatives, sup = self.valid_stat_scores.compute()\n",
    "        to_log = {'valid/roc_auc_epoch': self.roc_auc.compute().item(),\n",
    "                  'valid/f1_epoch': self.binary_f1.compute().item(),\n",
    "                  'valid/precision_epoch': (true_positives/(true_positives+false_positives)).item(),\n",
    "                  'valid/recall_epoch': (true_positives/(true_positives+false_negatives)).item(),\n",
    "                  'valid/specificity_epoch': (true_negatives/(true_negatives+false_positives)).item()}\n",
    "        self.log_dict(to_log) \n",
    "        self.binary_f1.reset()\n",
    "        self.roc_auc.reset()\n",
    "        self.valid_stat_scores.reset()\n",
    "        \n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #    loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "    #    metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "    #    self.log_dict(metrics)\n",
    "    #    return metrics\n",
    "    \n",
    "\n",
    "\n",
    "config_hparams_dict = dict(\n",
    "    lr=Config.lr,\n",
    "    warmup_lr=Config.warmup_lr,\n",
    "    warmup_epochs=Config.warmup_epochs,\n",
    "    t_max=Config.t_max,\n",
    "    min_lr=Config.min_lr,\n",
    "    weight_decay=Config.weight_decay,\n",
    "    pos_weight=Config.pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SCHzzGNTervd"
   },
   "outputs": [],
   "source": [
    "tb_logger = TensorBoardLogger(save_dir=\"tb_logs\", name=\"rsna-breast-model\")\n",
    "loggers = [x for x in [wandb_logger, tb_logger] if x is not None]\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    monitor=\"valid/f1_epoch\",\n",
    "    filename=\"ckpt_epoch={epoch:02d}_validf1={valid/f1_epoch:02.0f}\",\n",
    "    auto_insert_metric_name=False,\n",
    "    save_top_k=3,\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    "    every_n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imsqGEdPerve"
   },
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAEMsNwmerve"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Fold: 0 / 0 ========================================\n",
      "Train df len 43764, cancers 926, percent 0.021158943423818664\n",
      "Valid df len 10942, cancers 232, percent 0.021202705172728934\n",
      "In train dataset cancer will be upsampled to percent 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /notebooks/rsna-breast/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                       | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model             | EfficientNet               | 21.5 M\n",
      "1 | fc                | Linear                     | 1.0 K \n",
      "2 | train_loss        | BCEWithLogitsLoss          | 0     \n",
      "3 | valid_loss        | BCEWithLogitsLoss          | 0     \n",
      "4 | roc_auc           | BinaryAUROC                | 0     \n",
      "5 | pr_curve          | BinaryPrecisionRecallCurve | 0     \n",
      "6 | valid_stat_scores | BinaryStatScores           | 0     \n",
      "7 | train_stat_scores | BinaryStatScores           | 0     \n",
      "8 | binary_f1         | BinaryF1Score              | 0     \n",
      "-----------------------------------------------------------------\n",
      "21.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.5 M    Total params\n",
      "42.919    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.9/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5050210d9ba4c7c83b66ab9619e6c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "/tmp/ipykernel_239/1121316196.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, dtype=torch.float)\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# add stratified k folds when ready\n",
    "kfold = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=Config.splits_count, train_size=0.8)\n",
    "for fold_, (train_idx, valid_idx) in enumerate(kfold.split(df, df['cancer'].values)):\n",
    "    print(f\"{'='*40} Fold: {fold_} / {Config.splits_count-1} {'='*40}\")\n",
    "\n",
    "\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True).copy()\n",
    "    valid_df = df.loc[valid_idx].reset_index(drop=True).copy()\n",
    "        \n",
    "    if Config.debug:\n",
    "        train_df = train_df.iloc[:int(len(train_df)*Config.debug_data_use_only_percent), :]\n",
    "        valid_df = train_df.iloc[:int(len(valid_df)*Config.debug_data_use_only_percent), :]\n",
    "    \n",
    "    train_rows_weight = class_weight(train_df[\"cancer\"].values, Config.positive_upsample_to_percent)   \n",
    "\n",
    "    print(f\"Train df len {len(train_df)}, cancers {train_df['cancer'].sum()}, percent {train_df['cancer'].sum()/len(train_df)}\")\n",
    "    print(f\"Valid df len {len(valid_df)}, cancers {valid_df['cancer'].sum()}, percent {valid_df['cancer'].sum()/len(valid_df)}\")        \n",
    "    print(f\"In train dataset cancer will be upsampled to percent {train_rows_weight[train_rows_weight>1].sum()/train_rows_weight.sum()}\")\n",
    "    \n",
    "    train_dataset = RSNAData(\n",
    "        df = train_df,\n",
    "        img_folder = Config.train_imgs_path,\n",
    "        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
    "        resize_aspect_ratio = Config.resize_aspect_ratio,\n",
    "        transform=train_augments\n",
    "    )\n",
    "    valid_dataset = RSNAData(\n",
    "        df = valid_df,\n",
    "        img_folder = Config.train_imgs_path,\n",
    "        has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
    "        resize_aspect_ratio = Config.resize_aspect_ratio,\n",
    "        transform = valid_augments\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.train_bs,\n",
    "        sampler=torch.utils.data.sampler.WeightedRandomSampler(train_rows_weight, len(train_df)),\n",
    "        num_workers=Config.dataloader_workers_count,\n",
    "        collate_fn=mixed_collate_imgs_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=Config.train_bs,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.dataloader_workers_count,\n",
    "        collate_fn=mixed_collate_imgs_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    # CHECKME after introducing k-fold start with fresh copy of base model\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        model_name = Config.model_name\n",
    "        base_model = timm.create_model(model_name, \n",
    "                                       pretrained=False,\n",
    "                                       **Config.model_params\n",
    "                                      )  \n",
    "        if Config.overwrite_checkpoint_hparams:\n",
    "            hparams = config_hparams_dict\n",
    "        else:\n",
    "            hparams = {}\n",
    "        hparams[\"base_model\"]=base_model\n",
    "        model = RSNAModel.load_from_checkpoint(checkpoint_path, \n",
    "                                              **hparams)\n",
    "    else:   \n",
    "        # it turns that i have lower LB than other kagglers using efficientnet.\n",
    "        # i have forgotten to set the drop path rate. Here is the fixed:\n",
    "        # efficientnet_b2(pretrained=True, drop_rate = 0.3, drop_path_rate = 0.2)\n",
    "        model_name = Config.model_name\n",
    "        base_model = timm.create_model(model_name, \n",
    "                                       pretrained=Config.use_pretrained,\n",
    "                                       **Config.model_params\n",
    "                                      ) \n",
    "        model = RSNAModel(base_model,\n",
    "                          **config_hparams_dict)\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=Config.epochs_count,\n",
    "        accelerator=Config.accelerator,\n",
    "        devices=Config.num_devices,\n",
    "        precision=Config.precision,\n",
    "        logger = loggers,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyorDvHcervf"
   },
   "outputs": [],
   "source": [
    "train_dataset = RSNAData(\n",
    "    df = df,\n",
    "    img_folder = Config.train_imgs_path,\n",
    "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
    "    resize_aspect_ratio = Config.resize_aspect_ratio,\n",
    "    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n",
    "    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n",
    "    transform=train_augments\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.train_bs,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=mixed_collate_imgs_fn\n",
    ")\n",
    "minibatch = next(iter(train_loader))\n",
    "print(minibatch[0].shape)\n",
    "print(minibatch[0].mean((1,2,3)))\n",
    "print(minibatch[0].min())\n",
    "print(minibatch[0].max())\n",
    "minibatch[0] += 1.0\n",
    "minibatch[0] /= 2.0\n",
    "print(minibatch[0].mean((1,2,3)))\n",
    "\n",
    "display_batch(minibatch, maximgs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnZYQWyyervf"
   },
   "outputs": [],
   "source": [
    "x_dataset = RSNAData(\n",
    "    df = df,\n",
    "    img_folder = Config.train_imgs_path,\n",
    "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
    "    # img_folder = \"/kaggle/input/rsna-breast-png-roi/train_images/256/\",\n",
    "    #img_folder = \"/kaggle/input/rsna-cut-off-empty-space-from-images\",\n",
    "    transform=ToTensorV2(),\n",
    "    return_filepath=True\n",
    ")\n",
    "\n",
    "weighted_x_loader = DataLoader(\n",
    "    x_dataset,\n",
    "    batch_size=Config.train_bs,\n",
    "    sampler=torch.utils.data.sampler.WeightedRandomSampler(\n",
    "        class_weight(df[\"cancer\"].values, Config.positive_upsample_to_percent),\n",
    "        len(df),\n",
    "        replacement=False),\n",
    "    num_workers=Config.dataloader_workers_count,\n",
    "    collate_fn=mixed_collate_imgs_fn,\n",
    ")\n",
    "\n",
    "#if Config.debug or True:\n",
    "#    for minibatch in weighted_x_loader:\n",
    "#        imgs = minibatch[0]\n",
    "#        if torch.isnan(minibatch[0]).sum() > 0:\n",
    "#            raise ValueError(\"bad pixel values\")\n",
    "#        if torch.isnan(minibatch[1]).sum() > 0:\n",
    "#            raise ValueError(\"bad targets values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T--jksEGervg"
   },
   "outputs": [],
   "source": [
    "minibatch = next(iter(weighted_x_loader))\n",
    "display_batch(minibatch, maximgs=10,figsize=(32,32))\n",
    "minibatch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCUFst_Rervh"
   },
   "outputs": [],
   "source": [
    "v_dataset = RSNAData(\n",
    "    df = df,\n",
    "    img_folder = Config.train_imgs_path,\n",
    "    has_patient_folder_sturcture = Config.data_has_patient_folder_sturcture,\n",
    "    transform = valid_augments\n",
    ")\n",
    "v_loader = DataLoader(\n",
    "    v_dataset,\n",
    "    batch_size=Config.train_bs,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.dataloader_workers_count,\n",
    "    collate_fn=mixed_collate_imgs_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "minibatch = next(iter(v_loader))\n",
    "print(minibatch[0].shape)\n",
    "print(minibatch[0].mean((1,2,3)))\n",
    "print(minibatch[0].min())\n",
    "print(minibatch[0].max())\n",
    "print(torch.isnan(minibatch[0]).sum())\n",
    "minibatch[0] += 1.0\n",
    "minibatch[0] /= 2.0\n",
    "print(minibatch[0].mean((1,2,3)))\n",
    "    \n",
    "display_batch(minibatch, maximgs=20,figsize=(32,32))\n",
    "\n",
    "minibatch[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPqiKnVQervh"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=tb_logs/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
